{"timestamp":"2025-12-31T03:01:02.165010Z","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-12-31T03:01:02.166315Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/bookings_per_listing_airflow3.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-12-31T03:01:02.300040Z","level":"info","event":"Spark-Submit cmd: spark-submit --master local --name listings_bookings_join /opt/airflow/dags/bookings_per_listing_spark.py --listings_file /opt/airflow/tmp/airbnb_data/listings.csv.gz --bookings_file /opt/airflow/tmp/data/bookings/2025-12-31_0301/bookings.csv --output_path /opt/airflow/tmp/data/bookings_per_listing/2025-12-31_0301","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":473}
{"timestamp":"2025-12-31T03:01:02.535492Z","level":"info","event":"WARNING: Using incubator modules: jdk.incubator.vector","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.226780Z","level":"info","event":"Reading listings from /opt/airflow/tmp/airbnb_data/listings.csv.gz","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.226932Z","level":"info","event":"Reading bookings from /opt/airflow/tmp/data/bookings/2025-12-31_0301/bookings.csv","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.515169Z","level":"info","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.525545Z","level":"info","event":"25/12/31 03:01:04 INFO SparkContext: Running Spark version 4.1.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.527591Z","level":"info","event":"25/12/31 03:01:04 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.527853Z","level":"info","event":"25/12/31 03:01:04 INFO SparkContext: Java version 17.0.17+10-Debian-1deb12u1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.588743Z","level":"info","event":"25/12/31 03:01:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.627678Z","level":"info","event":"25/12/31 03:01:04 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.628067Z","level":"info","event":"25/12/31 03:01:04 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.628324Z","level":"info","event":"25/12/31 03:01:04 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.628634Z","level":"info","event":"25/12/31 03:01:04 INFO SparkContext: Submitted application: ListingsBookingsJoin","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.659645Z","level":"info","event":"25/12/31 03:01:04 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.659994Z","level":"info","event":"25/12/31 03:01:04 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.660413Z","level":"info","event":"25/12/31 03:01:04 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.660755Z","level":"info","event":"25/12/31 03:01:04 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.661213Z","level":"info","event":"25/12/31 03:01:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.842509Z","level":"info","event":"25/12/31 03:01:04 INFO Utils: Successfully started service 'sparkDriver' on port 42049.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.867311Z","level":"info","event":"25/12/31 03:01:04 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.874850Z","level":"info","event":"25/12/31 03:01:04 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.884613Z","level":"info","event":"25/12/31 03:01:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.885080Z","level":"info","event":"25/12/31 03:01:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.887391Z","level":"info","event":"25/12/31 03:01:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.902791Z","level":"info","event":"25/12/31 03:01:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2a6e5892-b0b0-443e-86cb-d12f7da04d8e","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.917589Z","level":"info","event":"25/12/31 03:01:04 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:04.993179Z","level":"info","event":"25/12/31 03:01:04 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.045852Z","level":"info","event":"25/12/31 03:01:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.087148Z","level":"info","event":"25/12/31 03:01:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.088867Z","level":"info","event":"25/12/31 03:01:05 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.089479Z","level":"info","event":"25/12/31 03:01:05 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.097740Z","level":"info","event":"25/12/31 03:01:05 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.097932Z","level":"info","event":"25/12/31 03:01:05 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.097972Z","level":"info","event":"25/12/31 03:01:05 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.098113Z","level":"info","event":"25/12/31 03:01:05 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.098380Z","level":"info","event":"25/12/31 03:01:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.177624Z","level":"info","event":"25/12/31 03:01:05 INFO Executor: Starting executor ID driver on host 3a15b831ef09","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.178276Z","level":"info","event":"25/12/31 03:01:05 INFO Executor: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.178616Z","level":"info","event":"25/12/31 03:01:05 INFO Executor: Java version 17.0.17+10-Debian-1deb12u1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.183006Z","level":"info","event":"25/12/31 03:01:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.183522Z","level":"info","event":"25/12/31 03:01:05 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1fb36cd3 for default.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.196361Z","level":"info","event":"25/12/31 03:01:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41649.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.200967Z","level":"info","event":"25/12/31 03:01:05 INFO NettyBlockTransferService: Server created on 3a15b831ef09:41649","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.202397Z","level":"info","event":"25/12/31 03:01:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.211794Z","level":"info","event":"25/12/31 03:01:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3a15b831ef09, 41649, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.216187Z","level":"info","event":"25/12/31 03:01:05 INFO BlockManagerMasterEndpoint: Registering block manager 3a15b831ef09:41649 with 434.4 MiB RAM, BlockManagerId(driver, 3a15b831ef09, 41649, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.218078Z","level":"info","event":"25/12/31 03:01:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3a15b831ef09, 41649, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:05.218888Z","level":"info","event":"25/12/31 03:01:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3a15b831ef09, 41649, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:06.799394Z","level":"info","event":"25/12/31 03:01:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:06.804510Z","level":"info","event":"25/12/31 03:01:06 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.244713Z","level":"info","event":"25/12/31 03:01:07 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.281859Z","level":"info","event":"25/12/31 03:01:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.299642Z","level":"info","event":"25/12/31 03:01:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 215.6 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.334316Z","level":"info","event":"25/12/31 03:01:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.9 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.338743Z","level":"info","event":"25/12/31 03:01:07 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.435802Z","level":"info","event":"25/12/31 03:01:07 INFO FileInputFormat: Total input files to process : 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.443433Z","level":"info","event":"25/12/31 03:01:07 INFO FileInputFormat: Total input files to process : 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.482032Z","level":"info","event":"25/12/31 03:01:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.489630Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.489968Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.490187Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.491015Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.492398Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Missing parents found for ResultStage 0: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.493137Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.505514Z","level":"info","event":"25/12/31 03:01:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.7 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.510887Z","level":"info","event":"25/12/31 03:01:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.511959Z","level":"info","event":"25/12/31 03:01:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.530427Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.532632Z","level":"info","event":"25/12/31 03:01:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.552446Z","level":"info","event":"25/12/31 03:01:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 9772 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.562176Z","level":"info","event":"25/12/31 03:01:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.606703Z","level":"info","event":"25/12/31 03:01:07 INFO BinaryFileRDD: Task (TID 0) input split: Paths:/opt/airflow/tmp/airbnb_data/listings.csv.gz:0+9744878","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.680396Z","level":"info","event":"25/12/31 03:01:07 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.735389Z","level":"info","event":"25/12/31 03:01:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2745 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.745746Z","level":"info","event":"25/12/31 03:01:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 202 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.747021Z","level":"info","event":"25/12/31 03:01:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.750491Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 249 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.752157Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.752538Z","level":"info","event":"25/12/31 03:01:07 INFO TaskSchedulerImpl: Canceling stage 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.753072Z","level":"info","event":"25/12/31 03:01:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.754439Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 272.25138 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.784352Z","level":"info","event":"25/12/31 03:01:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.785169Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.785288Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.785327Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.785628Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.786407Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Missing parents found for ResultStage 1: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.786817Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.790739Z","level":"info","event":"25/12/31 03:01:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.9 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.792607Z","level":"info","event":"25/12/31 03:01:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.794195Z","level":"info","event":"25/12/31 03:01:07 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.795640Z","level":"info","event":"25/12/31 03:01:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.795816Z","level":"info","event":"25/12/31 03:01:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.797330Z","level":"info","event":"25/12/31 03:01:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 9772 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.798042Z","level":"info","event":"25/12/31 03:01:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.810519Z","level":"info","event":"25/12/31 03:01:07 INFO BinaryFileRDD: Task (TID 1) input split: Paths:/opt/airflow/tmp/airbnb_data/listings.csv.gz:0+9744878","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:07.852468Z","level":"info","event":"25/12/31 03:01:07 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:08.519632Z","level":"info","event":"25/12/31 03:01:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2084 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:08.522945Z","level":"info","event":"25/12/31 03:01:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 726 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:08.523098Z","level":"info","event":"25/12/31 03:01:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:08.523924Z","level":"info","event":"25/12/31 03:01:08 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 735 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:08.526491Z","level":"info","event":"25/12/31 03:01:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:08.526882Z","level":"info","event":"25/12/31 03:01:08 INFO TaskSchedulerImpl: Canceling stage 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:08.526987Z","level":"info","event":"25/12/31 03:01:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:08.528064Z","level":"info","event":"25/12/31 03:01:08 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 743.563851 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:08.758679Z","level":"info","event":"25/12/31 03:01:08 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:08.831889Z","level":"info","event":"25/12/31 03:01:08 INFO InMemoryFileIndex: It took 38 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.152547Z","level":"info","event":"25/12/31 03:01:09 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.153739Z","level":"info","event":"25/12/31 03:01:09 INFO FileSourceStrategy: Post-Scan Filters: Set((length(trim(value#79, None)) > 0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.406089Z","level":"info","event":"25/12/31 03:01:09 INFO CodeGenerator: Code generated in 129.577475 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.411061Z","level":"info","event":"25/12/31 03:01:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 214.5 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.422879Z","level":"info","event":"25/12/31 03:01:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.423930Z","level":"info","event":"25/12/31 03:01:09 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.436549Z","level":"info","event":"25/12/31 03:01:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196632 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.462704Z","level":"info","event":"25/12/31 03:01:09 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.463348Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.463476Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.463720Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.463863Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.464250Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Missing parents found for ResultStage 2: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.464615Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.480524Z","level":"info","event":"25/12/31 03:01:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.485154Z","level":"info","event":"25/12/31 03:01:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.486278Z","level":"info","event":"25/12/31 03:01:09 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.487953Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.488291Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.494901Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.496037Z","level":"info","event":"25/12/31 03:01:09 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.527307Z","level":"info","event":"25/12/31 03:01:09 INFO CodeGenerator: Code generated in 11.914328 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.532029Z","level":"info","event":"25/12/31 03:01:09 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2025-12-31_0301/bookings.csv, range: 0-2328, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.545434Z","level":"info","event":"25/12/31 03:01:09 INFO CodeGenerator: Code generated in 8.891553 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.613409Z","level":"info","event":"25/12/31 03:01:09 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1770 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.619319Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 126 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.619769Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 154 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.620011Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.620109Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.620244Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSchedulerImpl: Canceling stage 2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.620504Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.620939Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 158.140438 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.639968Z","level":"info","event":"25/12/31 03:01:09 INFO CodeGenerator: Code generated in 8.738347 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.673466Z","level":"info","event":"25/12/31 03:01:09 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.673671Z","level":"info","event":"25/12/31 03:01:09 INFO FileSourceStrategy: Post-Scan Filters: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.680814Z","level":"info","event":"25/12/31 03:01:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.5 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.690827Z","level":"info","event":"25/12/31 03:01:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.692897Z","level":"info","event":"25/12/31 03:01:09 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.694401Z","level":"info","event":"25/12/31 03:01:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196632 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.723334Z","level":"info","event":"25/12/31 03:01:09 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.724489Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.724612Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.724649Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.724907Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.725571Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Missing parents found for ResultStage 3: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.725790Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.736952Z","level":"info","event":"25/12/31 03:01:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 22.9 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.741827Z","level":"info","event":"25/12/31 03:01:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.743043Z","level":"info","event":"25/12/31 03:01:09 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.743610Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.743750Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.744881Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.745717Z","level":"info","event":"25/12/31 03:01:09 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.768911Z","level":"info","event":"25/12/31 03:01:09 INFO CodeGenerator: Code generated in 6.029595 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.770891Z","level":"info","event":"25/12/31 03:01:09 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2025-12-31_0301/bookings.csv, range: 0-2328, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.841880Z","level":"info","event":"25/12/31 03:01:09 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1970 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.843647Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 99 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.843765Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSchedulerImpl: Removed TaskSet 3.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.844124Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 117 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.844238Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.844300Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSchedulerImpl: Canceling stage 3","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.844355Z","level":"info","event":"25/12/31 03:01:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.844587Z","level":"info","event":"25/12/31 03:01:09 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 121.185209 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.847444Z","level":"info","event":"Listings schema:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850525Z","level":"info","event":"root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850646Z","level":"info","event":"|-- id: long (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850679Z","level":"info","event":"|-- listing_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850697Z","level":"info","event":"|-- scrape_id: long (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850712Z","level":"info","event":"|-- last_scraped: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850726Z","level":"info","event":"|-- source: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850742Z","level":"info","event":"|-- name: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850756Z","level":"info","event":"|-- description: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850769Z","level":"info","event":"|-- neighborhood_overview: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850783Z","level":"info","event":"|-- picture_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850796Z","level":"info","event":"|-- host_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850809Z","level":"info","event":"|-- host_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850822Z","level":"info","event":"|-- host_name: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850835Z","level":"info","event":"|-- host_since: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850847Z","level":"info","event":"|-- host_location: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850861Z","level":"info","event":"|-- host_about: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850874Z","level":"info","event":"|-- host_response_time: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850887Z","level":"info","event":"|-- host_response_rate: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850900Z","level":"info","event":"|-- host_acceptance_rate: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850913Z","level":"info","event":"|-- host_is_superhost: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850926Z","level":"info","event":"|-- host_thumbnail_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850939Z","level":"info","event":"|-- host_picture_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850952Z","level":"info","event":"|-- host_neighbourhood: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850964Z","level":"info","event":"|-- host_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850977Z","level":"info","event":"|-- host_total_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.850990Z","level":"info","event":"|-- host_verifications: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851003Z","level":"info","event":"|-- host_has_profile_pic: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851016Z","level":"info","event":"|-- host_identity_verified: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851028Z","level":"info","event":"|-- neighbourhood: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851041Z","level":"info","event":"|-- neighbourhood_cleansed: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851054Z","level":"info","event":"|-- neighbourhood_group_cleansed: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851067Z","level":"info","event":"|-- latitude: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851079Z","level":"info","event":"|-- longitude: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851092Z","level":"info","event":"|-- property_type: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851106Z","level":"info","event":"|-- room_type: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851120Z","level":"info","event":"|-- accommodates: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851132Z","level":"info","event":"|-- bathrooms: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851145Z","level":"info","event":"|-- bathrooms_text: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851158Z","level":"info","event":"|-- bedrooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851170Z","level":"info","event":"|-- beds: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851183Z","level":"info","event":"|-- amenities: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851209Z","level":"info","event":"|-- price: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851224Z","level":"info","event":"|-- minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851238Z","level":"info","event":"|-- maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851251Z","level":"info","event":"|-- minimum_minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851264Z","level":"info","event":"|-- maximum_minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851277Z","level":"info","event":"|-- minimum_maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851289Z","level":"info","event":"|-- maximum_maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851302Z","level":"info","event":"|-- minimum_nights_avg_ntm: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851315Z","level":"info","event":"|-- maximum_nights_avg_ntm: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851327Z","level":"info","event":"|-- calendar_updated: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851340Z","level":"info","event":"|-- has_availability: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851353Z","level":"info","event":"|-- availability_30: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851365Z","level":"info","event":"|-- availability_60: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851378Z","level":"info","event":"|-- availability_90: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851391Z","level":"info","event":"|-- availability_365: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851403Z","level":"info","event":"|-- calendar_last_scraped: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851416Z","level":"info","event":"|-- number_of_reviews: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851428Z","level":"info","event":"|-- number_of_reviews_ltm: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851441Z","level":"info","event":"|-- number_of_reviews_l30d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851454Z","level":"info","event":"|-- availability_eoy: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851466Z","level":"info","event":"|-- number_of_reviews_ly: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851479Z","level":"info","event":"|-- estimated_occupancy_l365d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851491Z","level":"info","event":"|-- estimated_revenue_l365d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851504Z","level":"info","event":"|-- first_review: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851517Z","level":"info","event":"|-- last_review: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851539Z","level":"info","event":"|-- review_scores_rating: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851553Z","level":"info","event":"|-- review_scores_accuracy: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851566Z","level":"info","event":"|-- review_scores_cleanliness: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851578Z","level":"info","event":"|-- review_scores_checkin: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851643Z","level":"info","event":"|-- review_scores_communication: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851674Z","level":"info","event":"|-- review_scores_location: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851693Z","level":"info","event":"|-- review_scores_value: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851708Z","level":"info","event":"|-- license: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851723Z","level":"info","event":"|-- instant_bookable: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851736Z","level":"info","event":"|-- calculated_host_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851751Z","level":"info","event":"|-- calculated_host_listings_count_entire_homes: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851765Z","level":"info","event":"|-- calculated_host_listings_count_private_rooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851779Z","level":"info","event":"|-- calculated_host_listings_count_shared_rooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851792Z","level":"info","event":"|-- reviews_per_month: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851805Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851820Z","level":"info","event":"Bookings schema:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851834Z","level":"info","event":"root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851847Z","level":"info","event":"|-- booking_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851860Z","level":"info","event":"|-- listing_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851873Z","level":"info","event":"|-- user_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851886Z","level":"info","event":"|-- booking_time: timestamp (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851899Z","level":"info","event":"|-- status: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:09.851912Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.053423Z","level":"info","event":"25/12/31 03:01:10 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.053590Z","level":"info","event":"25/12/31 03:01:10 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(id#0L))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.055516Z","level":"info","event":"25/12/31 03:01:10 INFO FileSourceStrategy: Pushed Filters: IsNotNull(listing_id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.055658Z","level":"info","event":"25/12/31 03:01:10 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(listing_id#97))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.145175Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 8.102253 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.148052Z","level":"info","event":"25/12/31 03:01:10 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 214.4 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.158583Z","level":"info","event":"25/12/31 03:01:10 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 38.7 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.159949Z","level":"info","event":"25/12/31 03:01:10 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.163755Z","level":"info","event":"25/12/31 03:01:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196632 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.178103Z","level":"info","event":"25/12/31 03:01:10 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.178885Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.178978Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.179139Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.179326Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.179563Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Missing parents found for ResultStage 4: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.179761Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.181691Z","level":"info","event":"25/12/31 03:01:10 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.5 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.187925Z","level":"info","event":"25/12/31 03:01:10 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.189184Z","level":"info","event":"25/12/31 03:01:10 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.189912Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.189987Z","level":"info","event":"25/12/31 03:01:10 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.191029Z","level":"info","event":"25/12/31 03:01:10 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.191406Z","level":"info","event":"25/12/31 03:01:10 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.204950Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 7.806124 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.206868Z","level":"info","event":"25/12/31 03:01:10 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2025-12-31_0301/bookings.csv, range: 0-2328, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.216765Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 7.369569 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.238664Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 4.211823 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.334572Z","level":"info","event":"25/12/31 03:01:10 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2143 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.336017Z","level":"info","event":"25/12/31 03:01:10 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 145 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.336128Z","level":"info","event":"25/12/31 03:01:10 INFO TaskSchedulerImpl: Removed TaskSet 4.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.336248Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 156 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.336679Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.336739Z","level":"info","event":"25/12/31 03:01:10 INFO TaskSchedulerImpl: Canceling stage 4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.336808Z","level":"info","event":"25/12/31 03:01:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.337408Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 158.857909 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.354788Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 5.006447 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.357113Z","level":"info","event":"25/12/31 03:01:10 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 1025.0 KiB, free 433.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.360108Z","level":"info","event":"25/12/31 03:01:10 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 980.0 B, free 433.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.361119Z","level":"info","event":"25/12/31 03:01:10 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.370589Z","level":"info","event":"25/12/31 03:01:10 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.370719Z","level":"info","event":"25/12/31 03:01:10 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(id#0L))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.473011Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 43.126889 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.475921Z","level":"info","event":"25/12/31 03:01:10 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 215.1 KiB, free 432.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.488598Z","level":"info","event":"25/12/31 03:01:10 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 432.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.489536Z","level":"info","event":"25/12/31 03:01:10 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.490401Z","level":"info","event":"25/12/31 03:01:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13939182 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.528176Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Registering RDD 21 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.531397Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Got map stage job 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.531808Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Final stage: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.532098Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.532425Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.533266Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Missing parents found for ShuffleMapStage 5: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.533328Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.541585Z","level":"info","event":"25/12/31 03:01:10 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 55.7 KiB, free 432.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.549424Z","level":"info","event":"25/12/31 03:01:10 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 432.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.550744Z","level":"info","event":"25/12/31 03:01:10 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.552017Z","level":"info","event":"25/12/31 03:01:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.552166Z","level":"info","event":"25/12/31 03:01:10 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.553556Z","level":"info","event":"25/12/31 03:01:10 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10234 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.553933Z","level":"info","event":"25/12/31 03:01:10 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.591747Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 27.588425 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.609634Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 11.751285 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.615749Z","level":"info","event":"25/12/31 03:01:10 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.615967Z","level":"info","event":"25/12/31 03:01:10 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.616044Z","level":"info","event":"25/12/31 03:01:10 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.616150Z","level":"info","event":"25/12/31 03:01:10 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.616218Z","level":"info","event":"25/12/31 03:01:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.630730Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 2.996462 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.648962Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 3.847456 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.667819Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 9.1449 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.673458Z","level":"info","event":"25/12/31 03:01:10 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/airbnb_data/listings.csv.gz, range: 0-9744878, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.681655Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 6.037363 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.687372Z","level":"info","event":"25/12/31 03:01:10 INFO CodeGenerator: Code generated in 2.973604 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:10.732467Z","level":"info","event":"25/12/31 03:01:10 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.245868Z","level":"info","event":"25/12/31 03:01:11 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3512 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.247673Z","level":"info","event":"25/12/31 03:01:11 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 695 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.247795Z","level":"info","event":"25/12/31 03:01:11 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.250104Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 715 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.250300Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.251152Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: running: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.251428Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: waiting: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.251726Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: failed: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.262288Z","level":"info","event":"25/12/31 03:01:11 INFO ShufflePartitionsUtil: For shuffle(0, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.294406Z","level":"info","event":"25/12/31 03:01:11 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.295018Z","level":"info","event":"25/12/31 03:01:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.295099Z","level":"info","event":"25/12/31 03:01:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.295965Z","level":"info","event":"25/12/31 03:01:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.434063Z","level":"info","event":"25/12/31 03:01:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.447675Z","level":"info","event":"25/12/31 03:01:11 INFO CodeGenerator: Code generated in 8.738831 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.471097Z","level":"info","event":"25/12/31 03:01:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.472265Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.472376Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.472410Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.472434Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.472694Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: Missing parents found for ResultStage 7: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.472844Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.486950Z","level":"info","event":"25/12/31 03:01:11 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 281.1 KiB, free 432.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.492824Z","level":"info","event":"25/12/31 03:01:11 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 104.1 KiB, free 432.5 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.494281Z","level":"info","event":"25/12/31 03:01:11 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.494791Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.494887Z","level":"info","event":"25/12/31 03:01:11 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.499893Z","level":"info","event":"25/12/31 03:01:11 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (3a15b831ef09,executor driver, partition 0, NODE_LOCAL, 9978 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.500430Z","level":"info","event":"25/12/31 03:01:11 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.522495Z","level":"info","event":"25/12/31 03:01:11 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.522735Z","level":"info","event":"25/12/31 03:01:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.522832Z","level":"info","event":"25/12/31 03:01:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.522932Z","level":"info","event":"25/12/31 03:01:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.768841Z","level":"info","event":"25/12/31 03:01:11 INFO ShuffleBlockFetcherIterator: Getting 1 (1032.0 B) non-empty blocks including 1 (1032.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.770198Z","level":"info","event":"25/12/31 03:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.785987Z","level":"info","event":"25/12/31 03:01:11 INFO CodeGenerator: Code generated in 13.062802 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.966722Z","level":"info","event":"25/12/31 03:01:11 INFO FileOutputCommitter: Saved output of task 'attempt_20251231030111704445277945784572_0007_m_000000_6' to file:/opt/airflow/tmp/data/bookings_per_listing/2025-12-31_0301/_temporary/0/task_20251231030111704445277945784572_0007_m_000000","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.967506Z","level":"info","event":"25/12/31 03:01:11 INFO SparkHadoopMapRedUtil: attempt_20251231030111704445277945784572_0007_m_000000_6: Committed. Elapsed time: 47 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.975657Z","level":"info","event":"25/12/31 03:01:11 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 6837 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.976971Z","level":"info","event":"25/12/31 03:01:11 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 479 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.977180Z","level":"info","event":"25/12/31 03:01:11 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.978524Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 505 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.979122Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.979367Z","level":"info","event":"25/12/31 03:01:11 INFO TaskSchedulerImpl: Canceling stage 7","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.979433Z","level":"info","event":"25/12/31 03:01:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.980242Z","level":"info","event":"25/12/31 03:01:11 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 508.987416 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:11.982157Z","level":"info","event":"25/12/31 03:01:11 INFO FileFormatWriter: Start to commit write Job 641d167f-acac-4c63-be6b-108c9b97c966.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:12.319305Z","level":"info","event":"25/12/31 03:01:12 INFO FileFormatWriter: Write Job 641d167f-acac-4c63-be6b-108c9b97c966 committed. Elapsed time: 336 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:12.320958Z","level":"info","event":"25/12/31 03:01:12 INFO FileFormatWriter: Finished processing stats for write job 641d167f-acac-4c63-be6b-108c9b97c966.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:12.326463Z","level":"info","event":"Aggregated results written to /opt/airflow/tmp/data/bookings_per_listing/2025-12-31_0301","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:12.326970Z","level":"info","event":"25/12/31 03:01:12 INFO SparkContext: SparkContext is stopping with exitCode 0 from stop at NativeMethodAccessorImpl.java:0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:12.343365Z","level":"info","event":"25/12/31 03:01:12 INFO SparkUI: Stopped Spark web UI at http://3a15b831ef09:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:12.349348Z","level":"info","event":"25/12/31 03:01:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:12.357537Z","level":"info","event":"25/12/31 03:01:12 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:12.357699Z","level":"info","event":"25/12/31 03:01:12 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:12.359123Z","level":"info","event":"25/12/31 03:01:12 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:12.360292Z","level":"info","event":"25/12/31 03:01:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:01:12.364185Z","level":"info","event":"25/12/31 03:01:12 INFO SparkContext: Successfully stopped SparkContext (Uptime: 7857 ms)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
