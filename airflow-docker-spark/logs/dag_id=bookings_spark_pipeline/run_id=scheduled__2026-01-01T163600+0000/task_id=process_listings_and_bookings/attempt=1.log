{"timestamp":"2026-01-01T16:38:19.794170Z","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2026-01-01T16:38:19.795244Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/bookings_per_listing_airflow3.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2026-01-01T16:38:19.925362Z","level":"info","event":"Spark-Submit cmd: spark-submit --master local --name listings_bookings_join /opt/airflow/dags/bookings_per_listing_spark.py --listings_file /opt/airflow/tmp/airbnb_data/listings.csv.gz --bookings_file /opt/airflow/tmp/data/bookings/2026-01-01_1636/bookings.csv --output_path /opt/airflow/tmp/data/bookings_per_listing/2026-01-01_1636","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":473}
{"timestamp":"2026-01-01T16:38:20.178054Z","level":"info","event":"WARNING: Using incubator modules: jdk.incubator.vector","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.158623Z","level":"info","event":"Reading listings from /opt/airflow/tmp/airbnb_data/listings.csv.gz","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.158766Z","level":"info","event":"Reading bookings from /opt/airflow/tmp/data/bookings/2026-01-01_1636/bookings.csv","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.439773Z","level":"info","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.451789Z","level":"info","event":"26/01/01 16:38:22 INFO SparkContext: Running Spark version 4.1.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.454667Z","level":"info","event":"26/01/01 16:38:22 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.454995Z","level":"info","event":"26/01/01 16:38:22 INFO SparkContext: Java version 17.0.17+10-Debian-1deb12u1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.525806Z","level":"info","event":"26/01/01 16:38:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.585630Z","level":"info","event":"26/01/01 16:38:22 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.586596Z","level":"info","event":"26/01/01 16:38:22 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.587084Z","level":"info","event":"26/01/01 16:38:22 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.587608Z","level":"info","event":"26/01/01 16:38:22 INFO SparkContext: Submitted application: ListingsBookingsJoin","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.624195Z","level":"info","event":"26/01/01 16:38:22 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.624569Z","level":"info","event":"26/01/01 16:38:22 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.624902Z","level":"info","event":"26/01/01 16:38:22 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.625322Z","level":"info","event":"26/01/01 16:38:22 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.625895Z","level":"info","event":"26/01/01 16:38:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.873716Z","level":"info","event":"26/01/01 16:38:22 INFO Utils: Successfully started service 'sparkDriver' on port 40555.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.897411Z","level":"info","event":"26/01/01 16:38:22 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.907723Z","level":"info","event":"26/01/01 16:38:22 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.921550Z","level":"info","event":"26/01/01 16:38:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.922592Z","level":"info","event":"26/01/01 16:38:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.925957Z","level":"info","event":"26/01/01 16:38:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.947708Z","level":"info","event":"26/01/01 16:38:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f920e850-49ec-4dc4-a094-0852fbca3fd0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.965952Z","level":"info","event":"26/01/01 16:38:22 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.056507Z","level":"info","event":"26/01/01 16:38:23 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.116457Z","level":"info","event":"26/01/01 16:38:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.161791Z","level":"info","event":"26/01/01 16:38:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.163510Z","level":"info","event":"26/01/01 16:38:23 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.164004Z","level":"info","event":"26/01/01 16:38:23 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.174647Z","level":"info","event":"26/01/01 16:38:23 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.174854Z","level":"info","event":"26/01/01 16:38:23 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.175033Z","level":"info","event":"26/01/01 16:38:23 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.175118Z","level":"info","event":"26/01/01 16:38:23 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.175558Z","level":"info","event":"26/01/01 16:38:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.279475Z","level":"info","event":"26/01/01 16:38:23 INFO Executor: Starting executor ID driver on host 85fdc3f7643c","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.279877Z","level":"info","event":"26/01/01 16:38:23 INFO Executor: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.280108Z","level":"info","event":"26/01/01 16:38:23 INFO Executor: Java version 17.0.17+10-Debian-1deb12u1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.284030Z","level":"info","event":"26/01/01 16:38:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.284570Z","level":"info","event":"26/01/01 16:38:23 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7837e93b for default.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.302762Z","level":"info","event":"26/01/01 16:38:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34383.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.308507Z","level":"info","event":"26/01/01 16:38:23 INFO NettyBlockTransferService: Server created on 85fdc3f7643c:34383","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.310547Z","level":"info","event":"26/01/01 16:38:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.327994Z","level":"info","event":"26/01/01 16:38:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 85fdc3f7643c, 34383, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.334351Z","level":"info","event":"26/01/01 16:38:23 INFO BlockManagerMasterEndpoint: Registering block manager 85fdc3f7643c:34383 with 434.4 MiB RAM, BlockManagerId(driver, 85fdc3f7643c, 34383, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.337456Z","level":"info","event":"26/01/01 16:38:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 85fdc3f7643c, 34383, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.339618Z","level":"info","event":"26/01/01 16:38:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 85fdc3f7643c, 34383, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:25.420578Z","level":"info","event":"26/01/01 16:38:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:25.427055Z","level":"info","event":"26/01/01 16:38:25 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:25.974818Z","level":"info","event":"26/01/01 16:38:25 INFO InMemoryFileIndex: It took 37 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.034372Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.061595Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 215.6 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.108881Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.9 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.115899Z","level":"info","event":"26/01/01 16:38:26 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.246178Z","level":"info","event":"26/01/01 16:38:26 INFO FileInputFormat: Total input files to process : 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.261045Z","level":"info","event":"26/01/01 16:38:26 INFO FileInputFormat: Total input files to process : 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.317598Z","level":"info","event":"26/01/01 16:38:26 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.330769Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.331506Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.331983Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.334013Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.336503Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Missing parents found for ResultStage 0: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.338388Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.357668Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.7 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.360575Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.362276Z","level":"info","event":"26/01/01 16:38:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.382847Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.386399Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.423939Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 9772 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.438077Z","level":"info","event":"26/01/01 16:38:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.504951Z","level":"info","event":"26/01/01 16:38:26 INFO BinaryFileRDD: Task (TID 0) input split: Paths:/opt/airflow/tmp/airbnb_data/listings.csv.gz:0+9744878","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.592104Z","level":"info","event":"26/01/01 16:38:26 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.652437Z","level":"info","event":"26/01/01 16:38:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2702 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.661489Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 255 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.662987Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.666648Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 316 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.668596Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.668880Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSchedulerImpl: Canceling stage 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.669539Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.671532Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 353.569623 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.708094Z","level":"info","event":"26/01/01 16:38:26 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.709701Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.709902Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.710133Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.710639Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.711406Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Missing parents found for ResultStage 1: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.711773Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.716023Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.9 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.722984Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.724952Z","level":"info","event":"26/01/01 16:38:26 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.726961Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.727214Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.728917Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 9772 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.729762Z","level":"info","event":"26/01/01 16:38:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.745106Z","level":"info","event":"26/01/01 16:38:26 INFO BinaryFileRDD: Task (TID 1) input split: Paths:/opt/airflow/tmp/airbnb_data/listings.csv.gz:0+9744878","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.772509Z","level":"info","event":"26/01/01 16:38:26 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.550868Z","level":"info","event":"26/01/01 16:38:27 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2084 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.552952Z","level":"info","event":"26/01/01 16:38:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 824 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.553098Z","level":"info","event":"26/01/01 16:38:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.553545Z","level":"info","event":"26/01/01 16:38:27 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 840 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.553775Z","level":"info","event":"26/01/01 16:38:27 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.553882Z","level":"info","event":"26/01/01 16:38:27 INFO TaskSchedulerImpl: Canceling stage 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.553958Z","level":"info","event":"26/01/01 16:38:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.555034Z","level":"info","event":"26/01/01 16:38:27 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 846.682602 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.806625Z","level":"info","event":"26/01/01 16:38:27 INFO InMemoryFileIndex: It took 26 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.855972Z","level":"info","event":"26/01/01 16:38:27 INFO InMemoryFileIndex: It took 17 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.279873Z","level":"info","event":"26/01/01 16:38:28 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.281932Z","level":"info","event":"26/01/01 16:38:28 INFO FileSourceStrategy: Post-Scan Filters: Set((length(trim(value#79, None)) > 0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.628737Z","level":"info","event":"26/01/01 16:38:28 INFO CodeGenerator: Code generated in 179.963911 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.635632Z","level":"info","event":"26/01/01 16:38:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 214.5 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.645489Z","level":"info","event":"26/01/01 16:38:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.647490Z","level":"info","event":"26/01/01 16:38:28 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.661617Z","level":"info","event":"26/01/01 16:38:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196477 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.710988Z","level":"info","event":"26/01/01 16:38:28 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.713258Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.713883Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.714221Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.714753Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.716032Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Missing parents found for ResultStage 2: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.717265Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.737085Z","level":"info","event":"26/01/01 16:38:28 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.739039Z","level":"info","event":"26/01/01 16:38:28 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.740758Z","level":"info","event":"26/01/01 16:38:28 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.742289Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.742639Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.747343Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.748303Z","level":"info","event":"26/01/01 16:38:28 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.799024Z","level":"info","event":"26/01/01 16:38:28 INFO CodeGenerator: Code generated in 21.066756 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.807066Z","level":"info","event":"26/01/01 16:38:28 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2026-01-01_1636/bookings.csv, range: 0-2173, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.832688Z","level":"info","event":"26/01/01 16:38:28 INFO CodeGenerator: Code generated in 15.237163 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.911848Z","level":"info","event":"26/01/01 16:38:28 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1770 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.917133Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 172 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.917927Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.918581Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 199 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.918972Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.919147Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSchedulerImpl: Canceling stage 2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.919364Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.919792Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 208.609588 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.949547Z","level":"info","event":"26/01/01 16:38:28 INFO CodeGenerator: Code generated in 13.259885 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.989914Z","level":"info","event":"26/01/01 16:38:28 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.990102Z","level":"info","event":"26/01/01 16:38:28 INFO FileSourceStrategy: Post-Scan Filters: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.000502Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.5 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.015928Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.018135Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.019575Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196477 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.058445Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.059963Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.060195Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.060294Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.060879Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.061743Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Missing parents found for ResultStage 3: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.061882Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.083825Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 22.9 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.091608Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.093999Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.095497Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.095733Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.098544Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.100000Z","level":"info","event":"26/01/01 16:38:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.145135Z","level":"info","event":"26/01/01 16:38:29 INFO CodeGenerator: Code generated in 12.325639 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.149869Z","level":"info","event":"26/01/01 16:38:29 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2026-01-01_1636/bookings.csv, range: 0-2173, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.235459Z","level":"info","event":"26/01/01 16:38:29 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2013 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.238970Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 141 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.239252Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSchedulerImpl: Removed TaskSet 3.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.240477Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 178 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.240879Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.241038Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSchedulerImpl: Canceling stage 3","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.241125Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.241395Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 182.785652 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.248234Z","level":"info","event":"Listings schema:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.254402Z","level":"info","event":"root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.254641Z","level":"info","event":"|-- id: long (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.254737Z","level":"info","event":"|-- listing_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.254794Z","level":"info","event":"|-- scrape_id: long (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.254848Z","level":"info","event":"|-- last_scraped: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.254875Z","level":"info","event":"|-- source: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.254896Z","level":"info","event":"|-- name: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.254917Z","level":"info","event":"|-- description: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.254944Z","level":"info","event":"|-- neighborhood_overview: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.254972Z","level":"info","event":"|-- picture_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.254991Z","level":"info","event":"|-- host_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255046Z","level":"info","event":"|-- host_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255079Z","level":"info","event":"|-- host_name: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255110Z","level":"info","event":"|-- host_since: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255139Z","level":"info","event":"|-- host_location: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255172Z","level":"info","event":"|-- host_about: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255200Z","level":"info","event":"|-- host_response_time: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255230Z","level":"info","event":"|-- host_response_rate: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255256Z","level":"info","event":"|-- host_acceptance_rate: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255284Z","level":"info","event":"|-- host_is_superhost: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255310Z","level":"info","event":"|-- host_thumbnail_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255335Z","level":"info","event":"|-- host_picture_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255360Z","level":"info","event":"|-- host_neighbourhood: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255387Z","level":"info","event":"|-- host_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255417Z","level":"info","event":"|-- host_total_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255446Z","level":"info","event":"|-- host_verifications: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255465Z","level":"info","event":"|-- host_has_profile_pic: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255481Z","level":"info","event":"|-- host_identity_verified: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255498Z","level":"info","event":"|-- neighbourhood: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255517Z","level":"info","event":"|-- neighbourhood_cleansed: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255541Z","level":"info","event":"|-- neighbourhood_group_cleansed: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255567Z","level":"info","event":"|-- latitude: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255594Z","level":"info","event":"|-- longitude: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255614Z","level":"info","event":"|-- property_type: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255634Z","level":"info","event":"|-- room_type: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255662Z","level":"info","event":"|-- accommodates: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255688Z","level":"info","event":"|-- bathrooms: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255713Z","level":"info","event":"|-- bathrooms_text: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255731Z","level":"info","event":"|-- bedrooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255756Z","level":"info","event":"|-- beds: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255788Z","level":"info","event":"|-- amenities: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255814Z","level":"info","event":"|-- price: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255842Z","level":"info","event":"|-- minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255866Z","level":"info","event":"|-- maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255883Z","level":"info","event":"|-- minimum_minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255921Z","level":"info","event":"|-- maximum_minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255968Z","level":"info","event":"|-- minimum_maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.255999Z","level":"info","event":"|-- maximum_maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256020Z","level":"info","event":"|-- minimum_nights_avg_ntm: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256039Z","level":"info","event":"|-- maximum_nights_avg_ntm: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256056Z","level":"info","event":"|-- calendar_updated: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256073Z","level":"info","event":"|-- has_availability: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256091Z","level":"info","event":"|-- availability_30: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256115Z","level":"info","event":"|-- availability_60: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256132Z","level":"info","event":"|-- availability_90: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256150Z","level":"info","event":"|-- availability_365: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256167Z","level":"info","event":"|-- calendar_last_scraped: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256184Z","level":"info","event":"|-- number_of_reviews: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256200Z","level":"info","event":"|-- number_of_reviews_ltm: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256217Z","level":"info","event":"|-- number_of_reviews_l30d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256234Z","level":"info","event":"|-- availability_eoy: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256251Z","level":"info","event":"|-- number_of_reviews_ly: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256268Z","level":"info","event":"|-- estimated_occupancy_l365d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256286Z","level":"info","event":"|-- estimated_revenue_l365d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256304Z","level":"info","event":"|-- first_review: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256321Z","level":"info","event":"|-- last_review: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256345Z","level":"info","event":"|-- review_scores_rating: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256363Z","level":"info","event":"|-- review_scores_accuracy: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256379Z","level":"info","event":"|-- review_scores_cleanliness: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256396Z","level":"info","event":"|-- review_scores_checkin: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256413Z","level":"info","event":"|-- review_scores_communication: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256429Z","level":"info","event":"|-- review_scores_location: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256448Z","level":"info","event":"|-- review_scores_value: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256465Z","level":"info","event":"|-- license: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256482Z","level":"info","event":"|-- instant_bookable: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256499Z","level":"info","event":"|-- calculated_host_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256516Z","level":"info","event":"|-- calculated_host_listings_count_entire_homes: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256534Z","level":"info","event":"|-- calculated_host_listings_count_private_rooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256568Z","level":"info","event":"|-- calculated_host_listings_count_shared_rooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256594Z","level":"info","event":"|-- reviews_per_month: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256616Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256636Z","level":"info","event":"Bookings schema:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256661Z","level":"info","event":"root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256680Z","level":"info","event":"|-- booking_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256697Z","level":"info","event":"|-- listing_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256715Z","level":"info","event":"|-- user_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256733Z","level":"info","event":"|-- booking_time: timestamp (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256749Z","level":"info","event":"|-- status: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.256766Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.692795Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.693196Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(id#0L))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.696938Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(listing_id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.697446Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(listing_id#97))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.864045Z","level":"info","event":"26/01/01 16:38:29 INFO CodeGenerator: Code generated in 16.141902 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.870205Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 214.4 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.885257Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 38.7 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.887856Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.893438Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196477 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.913148Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.914086Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.914245Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.914344Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.914933Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.915395Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Missing parents found for ResultStage 4: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.915674Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.918599Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.4 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.925878Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.928055Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.929158Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.929374Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.931421Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.932673Z","level":"info","event":"26/01/01 16:38:29 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.960585Z","level":"info","event":"26/01/01 16:38:29 INFO CodeGenerator: Code generated in 16.20299 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.962840Z","level":"info","event":"26/01/01 16:38:29 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2026-01-01_1636/bookings.csv, range: 0-2173, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.981467Z","level":"info","event":"26/01/01 16:38:29 INFO CodeGenerator: Code generated in 14.771615 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.014993Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 7.430552 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.095716Z","level":"info","event":"26/01/01 16:38:30 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2120 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.097345Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 166 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.097542Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSchedulerImpl: Removed TaskSet 4.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.098197Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 181 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.098853Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.099023Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSchedulerImpl: Canceling stage 4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.099108Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.099327Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 186.008684 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.125315Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 11.185966 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.130906Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 1025.0 KiB, free 432.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.137335Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 943.0 B, free 432.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.139670Z","level":"info","event":"26/01/01 16:38:30 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.167470Z","level":"info","event":"26/01/01 16:38:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.167844Z","level":"info","event":"26/01/01 16:38:30 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(id#0L))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.419041Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 106.703151 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.428340Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 215.1 KiB, free 432.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.440669Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 432.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.443188Z","level":"info","event":"26/01/01 16:38:30 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.445551Z","level":"info","event":"26/01/01 16:38:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13939182 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.547448Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Registering RDD 21 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.556422Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Got map stage job 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.557284Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Final stage: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.558431Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.559319Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.561910Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Missing parents found for ShuffleMapStage 5: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.562404Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.586483Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 55.7 KiB, free 432.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.589288Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 432.5 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.591367Z","level":"info","event":"26/01/01 16:38:30 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.594699Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.595267Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.598348Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 10234 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.599307Z","level":"info","event":"26/01/01 16:38:30 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.699478Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 70.228842 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.752138Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 34.005548 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.770088Z","level":"info","event":"26/01/01 16:38:30 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.772011Z","level":"info","event":"26/01/01 16:38:30 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.772253Z","level":"info","event":"26/01/01 16:38:30 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.772343Z","level":"info","event":"26/01/01 16:38:30 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.772390Z","level":"info","event":"26/01/01 16:38:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.810978Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 10.026928 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.839031Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 10.465991 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.857591Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 9.300262 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.867047Z","level":"info","event":"26/01/01 16:38:30 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/airbnb_data/listings.csv.gz, range: 0-9744878, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.881090Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 10.245765 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.897995Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 7.749223 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.915165Z","level":"info","event":"26/01/01 16:38:30 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.623303Z","level":"info","event":"26/01/01 16:38:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3469 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.625302Z","level":"info","event":"26/01/01 16:38:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1028 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.625517Z","level":"info","event":"26/01/01 16:38:31 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.628051Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 1059 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.628607Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.629095Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: running: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.629590Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: waiting: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.630177Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: failed: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.640305Z","level":"info","event":"26/01/01 16:38:31 INFO ShufflePartitionsUtil: For shuffle(0, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.686451Z","level":"info","event":"26/01/01 16:38:31 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.687563Z","level":"info","event":"26/01/01 16:38:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.687724Z","level":"info","event":"26/01/01 16:38:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.688935Z","level":"info","event":"26/01/01 16:38:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.770611Z","level":"info","event":"26/01/01 16:38:31 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.797422Z","level":"info","event":"26/01/01 16:38:31 INFO CodeGenerator: Code generated in 16.563991 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.845505Z","level":"info","event":"26/01/01 16:38:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.847850Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.848041Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.848120Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.848375Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.849530Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Missing parents found for ResultStage 7: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.849892Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.874663Z","level":"info","event":"26/01/01 16:38:31 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 281.1 KiB, free 432.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.876674Z","level":"info","event":"26/01/01 16:38:31 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 104.1 KiB, free 432.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.877878Z","level":"info","event":"26/01/01 16:38:31 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.879573Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.879806Z","level":"info","event":"26/01/01 16:38:31 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.890042Z","level":"info","event":"26/01/01 16:38:31 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (85fdc3f7643c,executor driver, partition 0, NODE_LOCAL, 9978 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.890918Z","level":"info","event":"26/01/01 16:38:31 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.930602Z","level":"info","event":"26/01/01 16:38:31 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.930899Z","level":"info","event":"26/01/01 16:38:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.931058Z","level":"info","event":"26/01/01 16:38:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.932339Z","level":"info","event":"26/01/01 16:38:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.154885Z","level":"info","event":"26/01/01 16:38:32 INFO ShuffleBlockFetcherIterator: Getting 1 (1032.0 B) non-empty blocks including 1 (1032.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.157208Z","level":"info","event":"26/01/01 16:38:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.177843Z","level":"info","event":"26/01/01 16:38:32 INFO CodeGenerator: Code generated in 15.801338 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.379760Z","level":"info","event":"26/01/01 16:38:32 INFO FileOutputCommitter: Saved output of task 'attempt_202601011638311005400902381487317_0007_m_000000_6' to file:/opt/airflow/tmp/data/bookings_per_listing/2026-01-01_1636/_temporary/0/task_202601011638311005400902381487317_0007_m_000000","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.380636Z","level":"info","event":"26/01/01 16:38:32 INFO SparkHadoopMapRedUtil: attempt_202601011638311005400902381487317_0007_m_000000_6: Committed. Elapsed time: 90 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.385356Z","level":"info","event":"26/01/01 16:38:32 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 6794 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.387300Z","level":"info","event":"26/01/01 16:38:32 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 502 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.387448Z","level":"info","event":"26/01/01 16:38:32 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.388260Z","level":"info","event":"26/01/01 16:38:32 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 538 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.388742Z","level":"info","event":"26/01/01 16:38:32 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.389364Z","level":"info","event":"26/01/01 16:38:32 INFO TaskSchedulerImpl: Canceling stage 7","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.389528Z","level":"info","event":"26/01/01 16:38:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.390468Z","level":"info","event":"26/01/01 16:38:32 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 544.656236 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.391700Z","level":"info","event":"26/01/01 16:38:32 INFO FileFormatWriter: Start to commit write Job 7b008973-143b-4478-b35d-001b117682f0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.880267Z","level":"info","event":"26/01/01 16:38:32 INFO FileFormatWriter: Write Job 7b008973-143b-4478-b35d-001b117682f0 committed. Elapsed time: 487 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.882231Z","level":"info","event":"26/01/01 16:38:32 INFO FileFormatWriter: Finished processing stats for write job 7b008973-143b-4478-b35d-001b117682f0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.889268Z","level":"info","event":"Aggregated results written to /opt/airflow/tmp/data/bookings_per_listing/2026-01-01_1636","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.890690Z","level":"info","event":"26/01/01 16:38:32 INFO SparkContext: SparkContext is stopping with exitCode 0 from stop at NativeMethodAccessorImpl.java:0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.910708Z","level":"info","event":"26/01/01 16:38:32 INFO SparkUI: Stopped Spark web UI at http://85fdc3f7643c:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.918867Z","level":"info","event":"26/01/01 16:38:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.928811Z","level":"info","event":"26/01/01 16:38:32 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.929050Z","level":"info","event":"26/01/01 16:38:32 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.930934Z","level":"info","event":"26/01/01 16:38:32 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.932446Z","level":"info","event":"26/01/01 16:38:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.938971Z","level":"info","event":"26/01/01 16:38:32 INFO SparkContext: Successfully stopped SparkContext (Uptime: 10508 ms)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
