{"timestamp":"2026-01-01T16:38:19.792582Z","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2026-01-01T16:38:19.793794Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/bookings_per_listing_airflow3.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2026-01-01T16:38:19.926561Z","level":"info","event":"Spark-Submit cmd: spark-submit --master local --name listings_bookings_join /opt/airflow/dags/bookings_per_listing_spark.py --listings_file /opt/airflow/tmp/airbnb_data/listings.csv.gz --bookings_file /opt/airflow/tmp/data/bookings/2026-01-01_1637/bookings.csv --output_path /opt/airflow/tmp/data/bookings_per_listing/2026-01-01_1637","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":473}
{"timestamp":"2026-01-01T16:38:20.181057Z","level":"info","event":"WARNING: Using incubator modules: jdk.incubator.vector","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.184457Z","level":"info","event":"Reading listings from /opt/airflow/tmp/airbnb_data/listings.csv.gz","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.184584Z","level":"info","event":"Reading bookings from /opt/airflow/tmp/data/bookings/2026-01-01_1637/bookings.csv","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.457501Z","level":"info","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.468053Z","level":"info","event":"26/01/01 16:38:22 INFO SparkContext: Running Spark version 4.1.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.471732Z","level":"info","event":"26/01/01 16:38:22 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.472064Z","level":"info","event":"26/01/01 16:38:22 INFO SparkContext: Java version 17.0.17+10-Debian-1deb12u1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.559146Z","level":"info","event":"26/01/01 16:38:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.609708Z","level":"info","event":"26/01/01 16:38:22 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.610031Z","level":"info","event":"26/01/01 16:38:22 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.610258Z","level":"info","event":"26/01/01 16:38:22 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.610565Z","level":"info","event":"26/01/01 16:38:22 INFO SparkContext: Submitted application: ListingsBookingsJoin","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.648791Z","level":"info","event":"26/01/01 16:38:22 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.649240Z","level":"info","event":"26/01/01 16:38:22 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.649773Z","level":"info","event":"26/01/01 16:38:22 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.650260Z","level":"info","event":"26/01/01 16:38:22 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.650861Z","level":"info","event":"26/01/01 16:38:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.921596Z","level":"info","event":"26/01/01 16:38:22 INFO Utils: Successfully started service 'sparkDriver' on port 35005.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.940674Z","level":"info","event":"26/01/01 16:38:22 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.952588Z","level":"info","event":"26/01/01 16:38:22 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.964685Z","level":"info","event":"26/01/01 16:38:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.965138Z","level":"info","event":"26/01/01 16:38:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.968076Z","level":"info","event":"26/01/01 16:38:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:22.986476Z","level":"info","event":"26/01/01 16:38:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-033a167f-ccde-4727-8180-eeef7660f225","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.001896Z","level":"info","event":"26/01/01 16:38:23 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.093037Z","level":"info","event":"26/01/01 16:38:23 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.142110Z","level":"info","event":"26/01/01 16:38:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.152648Z","level":"info","event":"26/01/01 16:38:23 INFO Utils: Successfully started service 'SparkUI' on port 4041.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.208145Z","level":"info","event":"26/01/01 16:38:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.209513Z","level":"info","event":"26/01/01 16:38:23 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.210016Z","level":"info","event":"26/01/01 16:38:23 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.218424Z","level":"info","event":"26/01/01 16:38:23 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.218634Z","level":"info","event":"26/01/01 16:38:23 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.218961Z","level":"info","event":"26/01/01 16:38:23 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.219324Z","level":"info","event":"26/01/01 16:38:23 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.220103Z","level":"info","event":"26/01/01 16:38:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.322194Z","level":"info","event":"26/01/01 16:38:23 INFO Executor: Starting executor ID driver on host 85fdc3f7643c","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.323027Z","level":"info","event":"26/01/01 16:38:23 INFO Executor: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.323737Z","level":"info","event":"26/01/01 16:38:23 INFO Executor: Java version 17.0.17+10-Debian-1deb12u1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.332753Z","level":"info","event":"26/01/01 16:38:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.333973Z","level":"info","event":"26/01/01 16:38:23 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@469a460b for default.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.353239Z","level":"info","event":"26/01/01 16:38:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40863.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.357977Z","level":"info","event":"26/01/01 16:38:23 INFO NettyBlockTransferService: Server created on 85fdc3f7643c:40863","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.360162Z","level":"info","event":"26/01/01 16:38:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.379398Z","level":"info","event":"26/01/01 16:38:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 85fdc3f7643c, 40863, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.386078Z","level":"info","event":"26/01/01 16:38:23 INFO BlockManagerMasterEndpoint: Registering block manager 85fdc3f7643c:40863 with 434.4 MiB RAM, BlockManagerId(driver, 85fdc3f7643c, 40863, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.389317Z","level":"info","event":"26/01/01 16:38:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 85fdc3f7643c, 40863, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:23.390644Z","level":"info","event":"26/01/01 16:38:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 85fdc3f7643c, 40863, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:25.394698Z","level":"info","event":"26/01/01 16:38:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:25.400375Z","level":"info","event":"26/01/01 16:38:25 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:25.949935Z","level":"info","event":"26/01/01 16:38:25 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.000349Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.029763Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 215.6 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.082658Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.9 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.088518Z","level":"info","event":"26/01/01 16:38:26 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.214042Z","level":"info","event":"26/01/01 16:38:26 INFO FileInputFormat: Total input files to process : 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.225254Z","level":"info","event":"26/01/01 16:38:26 INFO FileInputFormat: Total input files to process : 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.272201Z","level":"info","event":"26/01/01 16:38:26 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.280568Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.281191Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.281413Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.282736Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.284913Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Missing parents found for ResultStage 0: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.286451Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.301531Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.7 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.310171Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.311462Z","level":"info","event":"26/01/01 16:38:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.327852Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.330734Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.363719Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 9772 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.380092Z","level":"info","event":"26/01/01 16:38:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.455148Z","level":"info","event":"26/01/01 16:38:26 INFO BinaryFileRDD: Task (TID 0) input split: Paths:/opt/airflow/tmp/airbnb_data/listings.csv.gz:0+9744878","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.570702Z","level":"info","event":"26/01/01 16:38:26 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.626416Z","level":"info","event":"26/01/01 16:38:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2702 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.636132Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 291 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.637788Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.642021Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 346 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.646022Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.646574Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSchedulerImpl: Canceling stage 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.647123Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.649048Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 376.490644 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.693100Z","level":"info","event":"26/01/01 16:38:26 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.694699Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.694899Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.695426Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.696035Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.697437Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Missing parents found for ResultStage 1: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.698080Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.700945Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.9 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.703080Z","level":"info","event":"26/01/01 16:38:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.704917Z","level":"info","event":"26/01/01 16:38:26 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.707466Z","level":"info","event":"26/01/01 16:38:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.707629Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.709548Z","level":"info","event":"26/01/01 16:38:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 9772 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.710430Z","level":"info","event":"26/01/01 16:38:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.726351Z","level":"info","event":"26/01/01 16:38:26 INFO BinaryFileRDD: Task (TID 1) input split: Paths:/opt/airflow/tmp/airbnb_data/listings.csv.gz:0+9744878","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:26.770897Z","level":"info","event":"26/01/01 16:38:26 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.672309Z","level":"info","event":"26/01/01 16:38:27 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2084 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.674547Z","level":"info","event":"26/01/01 16:38:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 966 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.674698Z","level":"info","event":"26/01/01 16:38:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.675390Z","level":"info","event":"26/01/01 16:38:27 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 976 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.675658Z","level":"info","event":"26/01/01 16:38:27 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.675717Z","level":"info","event":"26/01/01 16:38:27 INFO TaskSchedulerImpl: Canceling stage 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.675814Z","level":"info","event":"26/01/01 16:38:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.676867Z","level":"info","event":"26/01/01 16:38:27 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 983.674887 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.877386Z","level":"info","event":"26/01/01 16:38:27 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:27.900597Z","level":"info","event":"26/01/01 16:38:27 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.328283Z","level":"info","event":"26/01/01 16:38:28 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.330376Z","level":"info","event":"26/01/01 16:38:28 INFO FileSourceStrategy: Post-Scan Filters: Set((length(trim(value#79, None)) > 0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.699694Z","level":"info","event":"26/01/01 16:38:28 INFO CodeGenerator: Code generated in 186.254599 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.707383Z","level":"info","event":"26/01/01 16:38:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 214.5 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.720108Z","level":"info","event":"26/01/01 16:38:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.721960Z","level":"info","event":"26/01/01 16:38:28 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.737738Z","level":"info","event":"26/01/01 16:38:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196276 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.784676Z","level":"info","event":"26/01/01 16:38:28 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.785987Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.786320Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.786443Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.786873Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.787490Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Missing parents found for ResultStage 2: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.787860Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.810619Z","level":"info","event":"26/01/01 16:38:28 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.812929Z","level":"info","event":"26/01/01 16:38:28 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.814396Z","level":"info","event":"26/01/01 16:38:28 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.815960Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.816234Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.820066Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.821120Z","level":"info","event":"26/01/01 16:38:28 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.871392Z","level":"info","event":"26/01/01 16:38:28 INFO CodeGenerator: Code generated in 23.283363 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.877949Z","level":"info","event":"26/01/01 16:38:28 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2026-01-01_1637/bookings.csv, range: 0-1972, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.901963Z","level":"info","event":"26/01/01 16:38:28 INFO CodeGenerator: Code generated in 15.592898 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.977732Z","level":"info","event":"26/01/01 16:38:28 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1727 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.980661Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 162 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.980932Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.981946Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 191 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.982501Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.982770Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSchedulerImpl: Canceling stage 2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.982886Z","level":"info","event":"26/01/01 16:38:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:28.983286Z","level":"info","event":"26/01/01 16:38:28 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 198.473418 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.014985Z","level":"info","event":"26/01/01 16:38:29 INFO CodeGenerator: Code generated in 16.31486 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.051227Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.051541Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceStrategy: Post-Scan Filters: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.062823Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.5 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.072296Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.074897Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.076622Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196276 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.121128Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.129015Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.129409Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.129529Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.129754Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.130400Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Missing parents found for ResultStage 3: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.130841Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.155412Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 22.9 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.157795Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.160507Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.162121Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.162387Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.165107Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.166170Z","level":"info","event":"26/01/01 16:38:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.204898Z","level":"info","event":"26/01/01 16:38:29 INFO CodeGenerator: Code generated in 14.802139 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.208389Z","level":"info","event":"26/01/01 16:38:29 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2026-01-01_1637/bookings.csv, range: 0-1972, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.277929Z","level":"info","event":"26/01/01 16:38:29 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1970 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.282066Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 117 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.282864Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSchedulerImpl: Removed TaskSet 3.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.283030Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 151 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.283336Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.283469Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSchedulerImpl: Canceling stage 3","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.283536Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.283972Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 162.683808 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.291214Z","level":"info","event":"Listings schema:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.299922Z","level":"info","event":"root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300156Z","level":"info","event":"|-- id: long (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300259Z","level":"info","event":"|-- listing_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300334Z","level":"info","event":"|-- scrape_id: long (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300401Z","level":"info","event":"|-- last_scraped: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300517Z","level":"info","event":"|-- source: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300599Z","level":"info","event":"|-- name: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300647Z","level":"info","event":"|-- description: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300686Z","level":"info","event":"|-- neighborhood_overview: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300719Z","level":"info","event":"|-- picture_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300750Z","level":"info","event":"|-- host_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300781Z","level":"info","event":"|-- host_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300812Z","level":"info","event":"|-- host_name: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300843Z","level":"info","event":"|-- host_since: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300864Z","level":"info","event":"|-- host_location: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300882Z","level":"info","event":"|-- host_about: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300899Z","level":"info","event":"|-- host_response_time: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300917Z","level":"info","event":"|-- host_response_rate: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300935Z","level":"info","event":"|-- host_acceptance_rate: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.300984Z","level":"info","event":"|-- host_is_superhost: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301020Z","level":"info","event":"|-- host_thumbnail_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301054Z","level":"info","event":"|-- host_picture_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301085Z","level":"info","event":"|-- host_neighbourhood: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301119Z","level":"info","event":"|-- host_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301152Z","level":"info","event":"|-- host_total_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301182Z","level":"info","event":"|-- host_verifications: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301206Z","level":"info","event":"|-- host_has_profile_pic: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301236Z","level":"info","event":"|-- host_identity_verified: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301263Z","level":"info","event":"|-- neighbourhood: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301285Z","level":"info","event":"|-- neighbourhood_cleansed: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301314Z","level":"info","event":"|-- neighbourhood_group_cleansed: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301344Z","level":"info","event":"|-- latitude: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301376Z","level":"info","event":"|-- longitude: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301410Z","level":"info","event":"|-- property_type: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301445Z","level":"info","event":"|-- room_type: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301481Z","level":"info","event":"|-- accommodates: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301514Z","level":"info","event":"|-- bathrooms: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301544Z","level":"info","event":"|-- bathrooms_text: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301575Z","level":"info","event":"|-- bedrooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301607Z","level":"info","event":"|-- beds: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301637Z","level":"info","event":"|-- amenities: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301669Z","level":"info","event":"|-- price: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301690Z","level":"info","event":"|-- minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301718Z","level":"info","event":"|-- maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301749Z","level":"info","event":"|-- minimum_minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301798Z","level":"info","event":"|-- maximum_minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301832Z","level":"info","event":"|-- minimum_maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301862Z","level":"info","event":"|-- maximum_maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301892Z","level":"info","event":"|-- minimum_nights_avg_ntm: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301924Z","level":"info","event":"|-- maximum_nights_avg_ntm: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301955Z","level":"info","event":"|-- calendar_updated: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.301987Z","level":"info","event":"|-- has_availability: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302017Z","level":"info","event":"|-- availability_30: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302047Z","level":"info","event":"|-- availability_60: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302066Z","level":"info","event":"|-- availability_90: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302084Z","level":"info","event":"|-- availability_365: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302101Z","level":"info","event":"|-- calendar_last_scraped: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302179Z","level":"info","event":"|-- number_of_reviews: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302220Z","level":"info","event":"|-- number_of_reviews_ltm: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302250Z","level":"info","event":"|-- number_of_reviews_l30d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302281Z","level":"info","event":"|-- availability_eoy: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302307Z","level":"info","event":"|-- number_of_reviews_ly: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302333Z","level":"info","event":"|-- estimated_occupancy_l365d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302361Z","level":"info","event":"|-- estimated_revenue_l365d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302388Z","level":"info","event":"|-- first_review: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302416Z","level":"info","event":"|-- last_review: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302445Z","level":"info","event":"|-- review_scores_rating: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302472Z","level":"info","event":"|-- review_scores_accuracy: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302497Z","level":"info","event":"|-- review_scores_cleanliness: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302524Z","level":"info","event":"|-- review_scores_checkin: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302554Z","level":"info","event":"|-- review_scores_communication: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302587Z","level":"info","event":"|-- review_scores_location: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302616Z","level":"info","event":"|-- review_scores_value: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302648Z","level":"info","event":"|-- license: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302678Z","level":"info","event":"|-- instant_bookable: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302707Z","level":"info","event":"|-- calculated_host_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302737Z","level":"info","event":"|-- calculated_host_listings_count_entire_homes: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302768Z","level":"info","event":"|-- calculated_host_listings_count_private_rooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302813Z","level":"info","event":"|-- calculated_host_listings_count_shared_rooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302849Z","level":"info","event":"|-- reviews_per_month: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302884Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.302960Z","level":"info","event":"Bookings schema:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.303036Z","level":"info","event":"root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.303086Z","level":"info","event":"|-- booking_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.303125Z","level":"info","event":"|-- listing_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.303160Z","level":"info","event":"|-- user_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.303197Z","level":"info","event":"|-- booking_time: timestamp (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.303232Z","level":"info","event":"|-- status: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.303266Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.723954Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.724433Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(id#0L))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.729347Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(listing_id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.729865Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(listing_id#97))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.918863Z","level":"info","event":"26/01/01 16:38:29 INFO CodeGenerator: Code generated in 16.010525 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.923886Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 214.4 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.936806Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 38.7 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.940337Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.947155Z","level":"info","event":"26/01/01 16:38:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196276 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.975579Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.977115Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.977318Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.977414Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.980290Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.980649Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Missing parents found for ResultStage 4: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.980859Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.985286Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.4 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.987434Z","level":"info","event":"26/01/01 16:38:29 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.989905Z","level":"info","event":"26/01/01 16:38:29 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.991112Z","level":"info","event":"26/01/01 16:38:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.991331Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.993538Z","level":"info","event":"26/01/01 16:38:29 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:29.994317Z","level":"info","event":"26/01/01 16:38:29 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.021200Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 16.708868 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.022732Z","level":"info","event":"26/01/01 16:38:30 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2026-01-01_1637/bookings.csv, range: 0-1972, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.041105Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 15.570026 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.073419Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 8.649951 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.145645Z","level":"info","event":"26/01/01 16:38:30 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2035 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.148106Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 155 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.148481Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSchedulerImpl: Removed TaskSet 4.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.149048Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 166 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.149378Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.149495Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSchedulerImpl: Canceling stage 4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.149710Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.150210Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 174.35495 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.183658Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 14.221376 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.188371Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 1025.0 KiB, free 432.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.195307Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 885.0 B, free 432.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.197014Z","level":"info","event":"26/01/01 16:38:30 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.222756Z","level":"info","event":"26/01/01 16:38:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.223556Z","level":"info","event":"26/01/01 16:38:30 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(id#0L))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.454593Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 88.721482 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.458380Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 215.1 KiB, free 432.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.470187Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 432.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.472972Z","level":"info","event":"26/01/01 16:38:30 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.475144Z","level":"info","event":"26/01/01 16:38:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13939182 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.556575Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Registering RDD 21 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.563879Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Got map stage job 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.564815Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Final stage: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.565213Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.565746Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.567358Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Missing parents found for ShuffleMapStage 5: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.567637Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.586652Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 55.7 KiB, free 432.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.589944Z","level":"info","event":"26/01/01 16:38:30 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 432.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.591692Z","level":"info","event":"26/01/01 16:38:30 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.594034Z","level":"info","event":"26/01/01 16:38:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.594359Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.597436Z","level":"info","event":"26/01/01 16:38:30 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (85fdc3f7643c,executor driver, partition 0, PROCESS_LOCAL, 10234 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.600075Z","level":"info","event":"26/01/01 16:38:30 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.697046Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 67.11597 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.740659Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 30.523979 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.774399Z","level":"info","event":"26/01/01 16:38:30 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.774609Z","level":"info","event":"26/01/01 16:38:30 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.774699Z","level":"info","event":"26/01/01 16:38:30 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.774980Z","level":"info","event":"26/01/01 16:38:30 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.775288Z","level":"info","event":"26/01/01 16:38:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.807297Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 6.179644 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.827162Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 10.814683 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.851167Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 12.562887 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.859514Z","level":"info","event":"26/01/01 16:38:30 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/airbnb_data/listings.csv.gz, range: 0-9744878, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.878177Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 11.785217 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.890428Z","level":"info","event":"26/01/01 16:38:30 INFO CodeGenerator: Code generated in 8.378688 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:30.912988Z","level":"info","event":"26/01/01 16:38:30 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.591780Z","level":"info","event":"26/01/01 16:38:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3469 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.594067Z","level":"info","event":"26/01/01 16:38:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 998 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.595092Z","level":"info","event":"26/01/01 16:38:31 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.596786Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 1025 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.597378Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.597759Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: running: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.598378Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: waiting: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.598747Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: failed: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.612472Z","level":"info","event":"26/01/01 16:38:31 INFO ShufflePartitionsUtil: For shuffle(0, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.662015Z","level":"info","event":"26/01/01 16:38:31 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.663618Z","level":"info","event":"26/01/01 16:38:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.663853Z","level":"info","event":"26/01/01 16:38:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.666533Z","level":"info","event":"26/01/01 16:38:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.753462Z","level":"info","event":"26/01/01 16:38:31 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.774609Z","level":"info","event":"26/01/01 16:38:31 INFO CodeGenerator: Code generated in 13.72957 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.816771Z","level":"info","event":"26/01/01 16:38:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.819388Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.819623Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.819699Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.819846Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.820453Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Missing parents found for ResultStage 7: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.820686Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.845028Z","level":"info","event":"26/01/01 16:38:31 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 281.1 KiB, free 432.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.847483Z","level":"info","event":"26/01/01 16:38:31 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 104.1 KiB, free 432.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.849040Z","level":"info","event":"26/01/01 16:38:31 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.850216Z","level":"info","event":"26/01/01 16:38:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.850482Z","level":"info","event":"26/01/01 16:38:31 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.857549Z","level":"info","event":"26/01/01 16:38:31 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (85fdc3f7643c,executor driver, partition 0, NODE_LOCAL, 9978 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.858261Z","level":"info","event":"26/01/01 16:38:31 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.885077Z","level":"info","event":"26/01/01 16:38:31 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.885458Z","level":"info","event":"26/01/01 16:38:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.885573Z","level":"info","event":"26/01/01 16:38:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:31.886351Z","level":"info","event":"26/01/01 16:38:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.106555Z","level":"info","event":"26/01/01 16:38:32 INFO ShuffleBlockFetcherIterator: Getting 1 (1032.0 B) non-empty blocks including 1 (1032.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.110473Z","level":"info","event":"26/01/01 16:38:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.134938Z","level":"info","event":"26/01/01 16:38:32 INFO CodeGenerator: Code generated in 19.925354 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.211390Z","level":"info","event":"26/01/01 16:38:32 INFO FileOutputCommitter: Saved output of task 'attempt_202601011638316416449739503492680_0007_m_000000_6' to file:/opt/airflow/tmp/data/bookings_per_listing/2026-01-01_1637/_temporary/0/task_202601011638316416449739503492680_0007_m_000000","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.213349Z","level":"info","event":"26/01/01 16:38:32 INFO SparkHadoopMapRedUtil: attempt_202601011638316416449739503492680_0007_m_000000_6: Committed. Elapsed time: 23 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.218993Z","level":"info","event":"26/01/01 16:38:32 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 6794 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.221210Z","level":"info","event":"26/01/01 16:38:32 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 366 ms on 85fdc3f7643c (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.221452Z","level":"info","event":"26/01/01 16:38:32 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.222309Z","level":"info","event":"26/01/01 16:38:32 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 401 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.222628Z","level":"info","event":"26/01/01 16:38:32 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.223069Z","level":"info","event":"26/01/01 16:38:32 INFO TaskSchedulerImpl: Canceling stage 7","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.223311Z","level":"info","event":"26/01/01 16:38:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.224073Z","level":"info","event":"26/01/01 16:38:32 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 407.154714 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.225972Z","level":"info","event":"26/01/01 16:38:32 INFO FileFormatWriter: Start to commit write Job bbb9a3ea-4089-4c00-b6ba-e64c8be7b332.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.471105Z","level":"info","event":"26/01/01 16:38:32 INFO FileFormatWriter: Write Job bbb9a3ea-4089-4c00-b6ba-e64c8be7b332 committed. Elapsed time: 244 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.473823Z","level":"info","event":"26/01/01 16:38:32 INFO FileFormatWriter: Finished processing stats for write job bbb9a3ea-4089-4c00-b6ba-e64c8be7b332.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.483133Z","level":"info","event":"Aggregated results written to /opt/airflow/tmp/data/bookings_per_listing/2026-01-01_1637","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.484054Z","level":"info","event":"26/01/01 16:38:32 INFO SparkContext: SparkContext is stopping with exitCode 0 from stop at NativeMethodAccessorImpl.java:0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.510016Z","level":"info","event":"26/01/01 16:38:32 INFO SparkUI: Stopped Spark web UI at http://85fdc3f7643c:4041","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.522385Z","level":"info","event":"26/01/01 16:38:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.535326Z","level":"info","event":"26/01/01 16:38:32 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.535688Z","level":"info","event":"26/01/01 16:38:32 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.538571Z","level":"info","event":"26/01/01 16:38:32 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.540788Z","level":"info","event":"26/01/01 16:38:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2026-01-01T16:38:32.547633Z","level":"info","event":"26/01/01 16:38:32 INFO SparkContext: Successfully stopped SparkContext (Uptime: 10100 ms)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
