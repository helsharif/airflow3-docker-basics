{"timestamp":"2025-12-31T03:00:02.069932Z","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-12-31T03:00:02.071113Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/bookings_per_listing_airflow3.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-12-31T03:00:02.202745Z","level":"info","event":"Spark-Submit cmd: spark-submit --master local --name listings_bookings_join /opt/airflow/dags/bookings_per_listing_spark.py --listings_file /opt/airflow/tmp/airbnb_data/listings.csv.gz --bookings_file /opt/airflow/tmp/data/bookings/2025-12-31_0300/bookings.csv --output_path /opt/airflow/tmp/data/bookings_per_listing/2025-12-31_0300","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":473}
{"timestamp":"2025-12-31T03:00:02.431601Z","level":"info","event":"WARNING: Using incubator modules: jdk.incubator.vector","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:03.904058Z","level":"info","event":"Reading listings from /opt/airflow/tmp/airbnb_data/listings.csv.gz","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:03.904213Z","level":"info","event":"Reading bookings from /opt/airflow/tmp/data/bookings/2025-12-31_0300/bookings.csv","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.150584Z","level":"info","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.160247Z","level":"info","event":"25/12/31 03:00:04 INFO SparkContext: Running Spark version 4.1.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.162194Z","level":"info","event":"25/12/31 03:00:04 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.162458Z","level":"info","event":"25/12/31 03:00:04 INFO SparkContext: Java version 17.0.17+10-Debian-1deb12u1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.212735Z","level":"info","event":"25/12/31 03:00:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.247894Z","level":"info","event":"25/12/31 03:00:04 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.248318Z","level":"info","event":"25/12/31 03:00:04 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.248580Z","level":"info","event":"25/12/31 03:00:04 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.248850Z","level":"info","event":"25/12/31 03:00:04 INFO SparkContext: Submitted application: ListingsBookingsJoin","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.279164Z","level":"info","event":"25/12/31 03:00:04 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.279613Z","level":"info","event":"25/12/31 03:00:04 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.279857Z","level":"info","event":"25/12/31 03:00:04 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.280162Z","level":"info","event":"25/12/31 03:00:04 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.280592Z","level":"info","event":"25/12/31 03:00:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.438366Z","level":"info","event":"25/12/31 03:00:04 INFO Utils: Successfully started service 'sparkDriver' on port 37963.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.458410Z","level":"info","event":"25/12/31 03:00:04 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.466597Z","level":"info","event":"25/12/31 03:00:04 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.475992Z","level":"info","event":"25/12/31 03:00:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.476372Z","level":"info","event":"25/12/31 03:00:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.478465Z","level":"info","event":"25/12/31 03:00:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.491899Z","level":"info","event":"25/12/31 03:00:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d786694b-1494-49fd-afb0-ca216dd6f707","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.504238Z","level":"info","event":"25/12/31 03:00:04 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.582333Z","level":"info","event":"25/12/31 03:00:04 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.643163Z","level":"info","event":"25/12/31 03:00:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.686141Z","level":"info","event":"25/12/31 03:00:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.687262Z","level":"info","event":"25/12/31 03:00:04 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.687624Z","level":"info","event":"25/12/31 03:00:04 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.695928Z","level":"info","event":"25/12/31 03:00:04 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.696110Z","level":"info","event":"25/12/31 03:00:04 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.696344Z","level":"info","event":"25/12/31 03:00:04 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.696474Z","level":"info","event":"25/12/31 03:00:04 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.696794Z","level":"info","event":"25/12/31 03:00:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.777822Z","level":"info","event":"25/12/31 03:00:04 INFO Executor: Starting executor ID driver on host 3a15b831ef09","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.778237Z","level":"info","event":"25/12/31 03:00:04 INFO Executor: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.778477Z","level":"info","event":"25/12/31 03:00:04 INFO Executor: Java version 17.0.17+10-Debian-1deb12u1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.782594Z","level":"info","event":"25/12/31 03:00:04 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.783069Z","level":"info","event":"25/12/31 03:00:04 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7837e93b for default.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.796051Z","level":"info","event":"25/12/31 03:00:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33053.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.800434Z","level":"info","event":"25/12/31 03:00:04 INFO NettyBlockTransferService: Server created on 3a15b831ef09:33053","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.801719Z","level":"info","event":"25/12/31 03:00:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.811275Z","level":"info","event":"25/12/31 03:00:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3a15b831ef09, 33053, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.815473Z","level":"info","event":"25/12/31 03:00:04 INFO BlockManagerMasterEndpoint: Registering block manager 3a15b831ef09:33053 with 434.4 MiB RAM, BlockManagerId(driver, 3a15b831ef09, 33053, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.817423Z","level":"info","event":"25/12/31 03:00:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3a15b831ef09, 33053, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:04.818344Z","level":"info","event":"25/12/31 03:00:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3a15b831ef09, 33053, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:06.396269Z","level":"info","event":"25/12/31 03:00:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:06.400896Z","level":"info","event":"25/12/31 03:00:06 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:06.939338Z","level":"info","event":"25/12/31 03:00:06 INFO InMemoryFileIndex: It took 68 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:06.982249Z","level":"info","event":"25/12/31 03:00:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.001650Z","level":"info","event":"25/12/31 03:00:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 215.6 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.039130Z","level":"info","event":"25/12/31 03:00:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.9 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.043639Z","level":"info","event":"25/12/31 03:00:07 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.156360Z","level":"info","event":"25/12/31 03:00:07 INFO FileInputFormat: Total input files to process : 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.172352Z","level":"info","event":"25/12/31 03:00:07 INFO FileInputFormat: Total input files to process : 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.211904Z","level":"info","event":"25/12/31 03:00:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.219881Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.220204Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.220417Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.221615Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.222910Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Missing parents found for ResultStage 0: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.223818Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.235654Z","level":"info","event":"25/12/31 03:00:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.7 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.242439Z","level":"info","event":"25/12/31 03:00:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.243420Z","level":"info","event":"25/12/31 03:00:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.253525Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.255366Z","level":"info","event":"25/12/31 03:00:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.274062Z","level":"info","event":"25/12/31 03:00:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 9772 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.283667Z","level":"info","event":"25/12/31 03:00:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.337921Z","level":"info","event":"25/12/31 03:00:07 INFO BinaryFileRDD: Task (TID 0) input split: Paths:/opt/airflow/tmp/airbnb_data/listings.csv.gz:0+9744878","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.441553Z","level":"info","event":"25/12/31 03:00:07 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.487041Z","level":"info","event":"25/12/31 03:00:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2702 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.493192Z","level":"info","event":"25/12/31 03:00:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 228 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.494180Z","level":"info","event":"25/12/31 03:00:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.496684Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 265 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.498369Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.498483Z","level":"info","event":"25/12/31 03:00:07 INFO TaskSchedulerImpl: Canceling stage 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.498921Z","level":"info","event":"25/12/31 03:00:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.499950Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 287.995222 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.522545Z","level":"info","event":"25/12/31 03:00:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.523139Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.523225Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.523288Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.523685Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.524205Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Missing parents found for ResultStage 1: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.524482Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.526913Z","level":"info","event":"25/12/31 03:00:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.9 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.528463Z","level":"info","event":"25/12/31 03:00:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.529450Z","level":"info","event":"25/12/31 03:00:07 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.530348Z","level":"info","event":"25/12/31 03:00:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.530459Z","level":"info","event":"25/12/31 03:00:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.531455Z","level":"info","event":"25/12/31 03:00:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 9772 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.531961Z","level":"info","event":"25/12/31 03:00:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.541168Z","level":"info","event":"25/12/31 03:00:07 INFO BinaryFileRDD: Task (TID 1) input split: Paths:/opt/airflow/tmp/airbnb_data/listings.csv.gz:0+9744878","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:07.583699Z","level":"info","event":"25/12/31 03:00:07 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:08.314147Z","level":"info","event":"25/12/31 03:00:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2084 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:08.316438Z","level":"info","event":"25/12/31 03:00:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 785 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:08.316798Z","level":"info","event":"25/12/31 03:00:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:08.317415Z","level":"info","event":"25/12/31 03:00:08 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 792 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:08.317650Z","level":"info","event":"25/12/31 03:00:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:08.317823Z","level":"info","event":"25/12/31 03:00:08 INFO TaskSchedulerImpl: Canceling stage 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:08.317958Z","level":"info","event":"25/12/31 03:00:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:08.318787Z","level":"info","event":"25/12/31 03:00:08 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 796.211121 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:08.578915Z","level":"info","event":"25/12/31 03:00:08 INFO InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:08.681762Z","level":"info","event":"25/12/31 03:00:08 INFO InMemoryFileIndex: It took 50 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.065204Z","level":"info","event":"25/12/31 03:00:09 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.066781Z","level":"info","event":"25/12/31 03:00:09 INFO FileSourceStrategy: Post-Scan Filters: Set((length(trim(value#79, None)) > 0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.326216Z","level":"info","event":"25/12/31 03:00:09 INFO CodeGenerator: Code generated in 128.040946 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.331820Z","level":"info","event":"25/12/31 03:00:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 214.5 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.339208Z","level":"info","event":"25/12/31 03:00:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.341096Z","level":"info","event":"25/12/31 03:00:09 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.350882Z","level":"info","event":"25/12/31 03:00:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196282 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.382989Z","level":"info","event":"25/12/31 03:00:09 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.383956Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.384396Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.384508Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.384678Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.385364Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Missing parents found for ResultStage 2: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.385836Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.404530Z","level":"info","event":"25/12/31 03:00:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.406227Z","level":"info","event":"25/12/31 03:00:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.407558Z","level":"info","event":"25/12/31 03:00:09 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.408889Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.409467Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.413191Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.414289Z","level":"info","event":"25/12/31 03:00:09 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.455922Z","level":"info","event":"25/12/31 03:00:09 INFO CodeGenerator: Code generated in 16.126202 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.462314Z","level":"info","event":"25/12/31 03:00:09 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2025-12-31_0300/bookings.csv, range: 0-1978, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.480457Z","level":"info","event":"25/12/31 03:00:09 INFO CodeGenerator: Code generated in 12.6173 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.580498Z","level":"info","event":"25/12/31 03:00:09 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1727 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.582817Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 172 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.583066Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.583550Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 195 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.583830Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.583895Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSchedulerImpl: Canceling stage 2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.583923Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.584321Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 201.305043 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.605219Z","level":"info","event":"25/12/31 03:00:09 INFO CodeGenerator: Code generated in 10.055381 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.632582Z","level":"info","event":"25/12/31 03:00:09 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.632761Z","level":"info","event":"25/12/31 03:00:09 INFO FileSourceStrategy: Post-Scan Filters: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.639235Z","level":"info","event":"25/12/31 03:00:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.5 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.645666Z","level":"info","event":"25/12/31 03:00:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.647367Z","level":"info","event":"25/12/31 03:00:09 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.648345Z","level":"info","event":"25/12/31 03:00:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196282 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.671822Z","level":"info","event":"25/12/31 03:00:09 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.672792Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.672984Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.673053Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.673496Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.673995Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Missing parents found for ResultStage 3: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.674183Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.685540Z","level":"info","event":"25/12/31 03:00:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 22.9 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.686934Z","level":"info","event":"25/12/31 03:00:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.688026Z","level":"info","event":"25/12/31 03:00:09 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.688919Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.689156Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.690756Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.691562Z","level":"info","event":"25/12/31 03:00:09 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.714359Z","level":"info","event":"25/12/31 03:00:09 INFO CodeGenerator: Code generated in 8.776726 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.716540Z","level":"info","event":"25/12/31 03:00:09 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2025-12-31_0300/bookings.csv, range: 0-1978, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.813933Z","level":"info","event":"25/12/31 03:00:09 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1927 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.815420Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 125 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.815527Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSchedulerImpl: Removed TaskSet 3.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.815886Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 141 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.816015Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.816098Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSchedulerImpl: Canceling stage 3","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.816148Z","level":"info","event":"25/12/31 03:00:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.816359Z","level":"info","event":"25/12/31 03:00:09 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 144.564741 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.819462Z","level":"info","event":"Listings schema:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822206Z","level":"info","event":"root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822282Z","level":"info","event":"|-- id: long (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822310Z","level":"info","event":"|-- listing_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822328Z","level":"info","event":"|-- scrape_id: long (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822345Z","level":"info","event":"|-- last_scraped: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822359Z","level":"info","event":"|-- source: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822373Z","level":"info","event":"|-- name: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822387Z","level":"info","event":"|-- description: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822401Z","level":"info","event":"|-- neighborhood_overview: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822415Z","level":"info","event":"|-- picture_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822428Z","level":"info","event":"|-- host_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822442Z","level":"info","event":"|-- host_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822455Z","level":"info","event":"|-- host_name: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822468Z","level":"info","event":"|-- host_since: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822482Z","level":"info","event":"|-- host_location: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822495Z","level":"info","event":"|-- host_about: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822508Z","level":"info","event":"|-- host_response_time: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822520Z","level":"info","event":"|-- host_response_rate: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822534Z","level":"info","event":"|-- host_acceptance_rate: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822547Z","level":"info","event":"|-- host_is_superhost: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822560Z","level":"info","event":"|-- host_thumbnail_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822573Z","level":"info","event":"|-- host_picture_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822586Z","level":"info","event":"|-- host_neighbourhood: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822598Z","level":"info","event":"|-- host_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822635Z","level":"info","event":"|-- host_total_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822661Z","level":"info","event":"|-- host_verifications: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822678Z","level":"info","event":"|-- host_has_profile_pic: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822694Z","level":"info","event":"|-- host_identity_verified: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822708Z","level":"info","event":"|-- neighbourhood: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822721Z","level":"info","event":"|-- neighbourhood_cleansed: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822735Z","level":"info","event":"|-- neighbourhood_group_cleansed: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822748Z","level":"info","event":"|-- latitude: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822762Z","level":"info","event":"|-- longitude: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822775Z","level":"info","event":"|-- property_type: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822789Z","level":"info","event":"|-- room_type: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822802Z","level":"info","event":"|-- accommodates: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822815Z","level":"info","event":"|-- bathrooms: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822828Z","level":"info","event":"|-- bathrooms_text: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822841Z","level":"info","event":"|-- bedrooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822854Z","level":"info","event":"|-- beds: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822866Z","level":"info","event":"|-- amenities: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822879Z","level":"info","event":"|-- price: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822892Z","level":"info","event":"|-- minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822919Z","level":"info","event":"|-- maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822936Z","level":"info","event":"|-- minimum_minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822951Z","level":"info","event":"|-- maximum_minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.822984Z","level":"info","event":"|-- minimum_maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823003Z","level":"info","event":"|-- maximum_maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823017Z","level":"info","event":"|-- minimum_nights_avg_ntm: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823030Z","level":"info","event":"|-- maximum_nights_avg_ntm: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823043Z","level":"info","event":"|-- calendar_updated: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823056Z","level":"info","event":"|-- has_availability: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823071Z","level":"info","event":"|-- availability_30: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823085Z","level":"info","event":"|-- availability_60: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823098Z","level":"info","event":"|-- availability_90: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823112Z","level":"info","event":"|-- availability_365: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823125Z","level":"info","event":"|-- calendar_last_scraped: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823139Z","level":"info","event":"|-- number_of_reviews: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823152Z","level":"info","event":"|-- number_of_reviews_ltm: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823165Z","level":"info","event":"|-- number_of_reviews_l30d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823178Z","level":"info","event":"|-- availability_eoy: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823191Z","level":"info","event":"|-- number_of_reviews_ly: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823204Z","level":"info","event":"|-- estimated_occupancy_l365d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823218Z","level":"info","event":"|-- estimated_revenue_l365d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823231Z","level":"info","event":"|-- first_review: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823243Z","level":"info","event":"|-- last_review: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823266Z","level":"info","event":"|-- review_scores_rating: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823280Z","level":"info","event":"|-- review_scores_accuracy: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823293Z","level":"info","event":"|-- review_scores_cleanliness: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823306Z","level":"info","event":"|-- review_scores_checkin: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823319Z","level":"info","event":"|-- review_scores_communication: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823334Z","level":"info","event":"|-- review_scores_location: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823347Z","level":"info","event":"|-- review_scores_value: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823360Z","level":"info","event":"|-- license: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823373Z","level":"info","event":"|-- instant_bookable: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823386Z","level":"info","event":"|-- calculated_host_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823399Z","level":"info","event":"|-- calculated_host_listings_count_entire_homes: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823413Z","level":"info","event":"|-- calculated_host_listings_count_private_rooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823426Z","level":"info","event":"|-- calculated_host_listings_count_shared_rooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823439Z","level":"info","event":"|-- reviews_per_month: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823453Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823467Z","level":"info","event":"Bookings schema:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823481Z","level":"info","event":"root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823493Z","level":"info","event":"|-- booking_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823507Z","level":"info","event":"|-- listing_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823521Z","level":"info","event":"|-- user_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823534Z","level":"info","event":"|-- booking_time: timestamp (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823547Z","level":"info","event":"|-- status: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:09.823560Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.031377Z","level":"info","event":"25/12/31 03:00:10 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.031674Z","level":"info","event":"25/12/31 03:00:10 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(id#0L))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.033884Z","level":"info","event":"25/12/31 03:00:10 INFO FileSourceStrategy: Pushed Filters: IsNotNull(listing_id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.034070Z","level":"info","event":"25/12/31 03:00:10 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(listing_id#97))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.121234Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 7.685513 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.123807Z","level":"info","event":"25/12/31 03:00:10 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 214.4 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.129138Z","level":"info","event":"25/12/31 03:00:10 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 38.7 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.130481Z","level":"info","event":"25/12/31 03:00:10 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.133667Z","level":"info","event":"25/12/31 03:00:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196282 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.145017Z","level":"info","event":"25/12/31 03:00:10 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.145934Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.146037Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.146097Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.146249Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.146603Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Missing parents found for ResultStage 4: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.146742Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.149560Z","level":"info","event":"25/12/31 03:00:10 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.4 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.150788Z","level":"info","event":"25/12/31 03:00:10 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.151762Z","level":"info","event":"25/12/31 03:00:10 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.152345Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.152591Z","level":"info","event":"25/12/31 03:00:10 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.153661Z","level":"info","event":"25/12/31 03:00:10 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.154188Z","level":"info","event":"25/12/31 03:00:10 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.168755Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 8.715656 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.170029Z","level":"info","event":"25/12/31 03:00:10 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2025-12-31_0300/bookings.csv, range: 0-1978, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.180833Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 8.480393 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.199859Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 4.133454 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.297589Z","level":"info","event":"25/12/31 03:00:10 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2023 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.299215Z","level":"info","event":"25/12/31 03:00:10 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 145 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.299553Z","level":"info","event":"25/12/31 03:00:10 INFO TaskSchedulerImpl: Removed TaskSet 4.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.300182Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 152 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.300477Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.300596Z","level":"info","event":"25/12/31 03:00:10 INFO TaskSchedulerImpl: Canceling stage 4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.300663Z","level":"info","event":"25/12/31 03:00:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.300896Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 155.729232 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.320194Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 7.169915 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.323266Z","level":"info","event":"25/12/31 03:00:10 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 1025.0 KiB, free 432.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.327892Z","level":"info","event":"25/12/31 03:00:10 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 865.0 B, free 432.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.329487Z","level":"info","event":"25/12/31 03:00:10 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.341839Z","level":"info","event":"25/12/31 03:00:10 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.342136Z","level":"info","event":"25/12/31 03:00:10 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(id#0L))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.478272Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 53.717277 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.484242Z","level":"info","event":"25/12/31 03:00:10 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 215.1 KiB, free 432.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.492832Z","level":"info","event":"25/12/31 03:00:10 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 432.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.494571Z","level":"info","event":"25/12/31 03:00:10 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.495888Z","level":"info","event":"25/12/31 03:00:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13939182 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.544954Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Registering RDD 21 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.549449Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Got map stage job 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.549884Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Final stage: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.550169Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.550542Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.551703Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Missing parents found for ShuffleMapStage 5: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.551853Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.563154Z","level":"info","event":"25/12/31 03:00:10 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 55.7 KiB, free 432.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.565456Z","level":"info","event":"25/12/31 03:00:10 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 432.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.566756Z","level":"info","event":"25/12/31 03:00:10 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.568628Z","level":"info","event":"25/12/31 03:00:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.568753Z","level":"info","event":"25/12/31 03:00:10 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.571476Z","level":"info","event":"25/12/31 03:00:10 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10234 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.572298Z","level":"info","event":"25/12/31 03:00:10 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.624689Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 40.907537 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.651098Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 18.963418 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.672096Z","level":"info","event":"25/12/31 03:00:10 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.672347Z","level":"info","event":"25/12/31 03:00:10 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.672434Z","level":"info","event":"25/12/31 03:00:10 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.672490Z","level":"info","event":"25/12/31 03:00:10 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.672965Z","level":"info","event":"25/12/31 03:00:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.698761Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 4.52133 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.707534Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 4.584243 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.716947Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 5.044364 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.722711Z","level":"info","event":"25/12/31 03:00:10 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/airbnb_data/listings.csv.gz, range: 0-9744878, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.732602Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 7.876361 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.738882Z","level":"info","event":"25/12/31 03:00:10 INFO CodeGenerator: Code generated in 2.999722 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:10.785161Z","level":"info","event":"25/12/31 03:00:10 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.341741Z","level":"info","event":"25/12/31 03:00:11 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3469 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.342828Z","level":"info","event":"25/12/31 03:00:11 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 773 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.343253Z","level":"info","event":"25/12/31 03:00:11 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.345193Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 790 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.345581Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.345979Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: running: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.346240Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: waiting: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.346607Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: failed: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.352819Z","level":"info","event":"25/12/31 03:00:11 INFO ShufflePartitionsUtil: For shuffle(0, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.389700Z","level":"info","event":"25/12/31 03:00:11 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.390486Z","level":"info","event":"25/12/31 03:00:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.390601Z","level":"info","event":"25/12/31 03:00:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.391490Z","level":"info","event":"25/12/31 03:00:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.530969Z","level":"info","event":"25/12/31 03:00:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.548344Z","level":"info","event":"25/12/31 03:00:11 INFO CodeGenerator: Code generated in 10.81189 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.578743Z","level":"info","event":"25/12/31 03:00:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.580580Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.580791Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.580906Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.580976Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.581532Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: Missing parents found for ResultStage 7: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.581734Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.599221Z","level":"info","event":"25/12/31 03:00:11 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 281.1 KiB, free 432.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.600865Z","level":"info","event":"25/12/31 03:00:11 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 104.1 KiB, free 432.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.602275Z","level":"info","event":"25/12/31 03:00:11 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.602822Z","level":"info","event":"25/12/31 03:00:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.602986Z","level":"info","event":"25/12/31 03:00:11 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.608078Z","level":"info","event":"25/12/31 03:00:11 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (3a15b831ef09,executor driver, partition 0, NODE_LOCAL, 9978 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.608767Z","level":"info","event":"25/12/31 03:00:11 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.632239Z","level":"info","event":"25/12/31 03:00:11 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.632500Z","level":"info","event":"25/12/31 03:00:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.632588Z","level":"info","event":"25/12/31 03:00:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.632853Z","level":"info","event":"25/12/31 03:00:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.946713Z","level":"info","event":"25/12/31 03:00:11 INFO ShuffleBlockFetcherIterator: Getting 1 (903.0 B) non-empty blocks including 1 (903.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.948030Z","level":"info","event":"25/12/31 03:00:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:11.962557Z","level":"info","event":"25/12/31 03:00:11 INFO CodeGenerator: Code generated in 11.58915 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:12.194793Z","level":"info","event":"25/12/31 03:00:12 INFO FileOutputCommitter: Saved output of task 'attempt_202512310300112752915658761544757_0007_m_000000_6' to file:/opt/airflow/tmp/data/bookings_per_listing/2025-12-31_0300/_temporary/0/task_202512310300112752915658761544757_0007_m_000000","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:12.196140Z","level":"info","event":"25/12/31 03:00:12 INFO SparkHadoopMapRedUtil: attempt_202512310300112752915658761544757_0007_m_000000_6: Committed. Elapsed time: 137 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:12.201262Z","level":"info","event":"25/12/31 03:00:12 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 6794 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:12.202768Z","level":"info","event":"25/12/31 03:00:12 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 597 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:12.202873Z","level":"info","event":"25/12/31 03:00:12 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:12.203435Z","level":"info","event":"25/12/31 03:00:12 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 621 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:12.203593Z","level":"info","event":"25/12/31 03:00:12 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:12.203657Z","level":"info","event":"25/12/31 03:00:12 INFO TaskSchedulerImpl: Canceling stage 7","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:12.203707Z","level":"info","event":"25/12/31 03:00:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:12.204172Z","level":"info","event":"25/12/31 03:00:12 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 625.294036 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:12.205404Z","level":"info","event":"25/12/31 03:00:12 INFO FileFormatWriter: Start to commit write Job 6d7278ac-55b1-4068-be5f-89bb339842cd.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:13.021808Z","level":"info","event":"25/12/31 03:00:13 INFO FileFormatWriter: Write Job 6d7278ac-55b1-4068-be5f-89bb339842cd committed. Elapsed time: 815 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:13.024232Z","level":"info","event":"25/12/31 03:00:13 INFO FileFormatWriter: Finished processing stats for write job 6d7278ac-55b1-4068-be5f-89bb339842cd.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:13.030352Z","level":"info","event":"Aggregated results written to /opt/airflow/tmp/data/bookings_per_listing/2025-12-31_0300","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:13.030852Z","level":"info","event":"25/12/31 03:00:13 INFO SparkContext: SparkContext is stopping with exitCode 0 from stop at NativeMethodAccessorImpl.java:0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:13.049510Z","level":"info","event":"25/12/31 03:00:13 INFO SparkUI: Stopped Spark web UI at http://3a15b831ef09:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:13.056350Z","level":"info","event":"25/12/31 03:00:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:13.065141Z","level":"info","event":"25/12/31 03:00:13 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:13.065317Z","level":"info","event":"25/12/31 03:00:13 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:13.067077Z","level":"info","event":"25/12/31 03:00:13 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:13.068363Z","level":"info","event":"25/12/31 03:00:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:00:13.072691Z","level":"info","event":"25/12/31 03:00:13 INFO SparkContext: Successfully stopped SparkContext (Uptime: 8929 ms)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
