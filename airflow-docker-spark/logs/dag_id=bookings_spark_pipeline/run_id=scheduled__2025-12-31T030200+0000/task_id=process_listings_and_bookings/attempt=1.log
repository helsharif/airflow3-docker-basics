{"timestamp":"2025-12-31T03:02:01.285375Z","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-12-31T03:02:01.286426Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/bookings_per_listing_airflow3.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-12-31T03:02:01.401499Z","level":"info","event":"Spark-Submit cmd: spark-submit --master local --name listings_bookings_join /opt/airflow/dags/bookings_per_listing_spark.py --listings_file /opt/airflow/tmp/airbnb_data/listings.csv.gz --bookings_file /opt/airflow/tmp/data/bookings/2025-12-31_0302/bookings.csv --output_path /opt/airflow/tmp/data/bookings_per_listing/2025-12-31_0302","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":473}
{"timestamp":"2025-12-31T03:02:01.616670Z","level":"info","event":"WARNING: Using incubator modules: jdk.incubator.vector","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.101378Z","level":"info","event":"Reading listings from /opt/airflow/tmp/airbnb_data/listings.csv.gz","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.101499Z","level":"info","event":"Reading bookings from /opt/airflow/tmp/data/bookings/2025-12-31_0302/bookings.csv","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.336804Z","level":"info","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.346093Z","level":"info","event":"25/12/31 03:02:03 INFO SparkContext: Running Spark version 4.1.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.348066Z","level":"info","event":"25/12/31 03:02:03 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.348287Z","level":"info","event":"25/12/31 03:02:03 INFO SparkContext: Java version 17.0.17+10-Debian-1deb12u1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.405797Z","level":"info","event":"25/12/31 03:02:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.439052Z","level":"info","event":"25/12/31 03:02:03 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.439424Z","level":"info","event":"25/12/31 03:02:03 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.439549Z","level":"info","event":"25/12/31 03:02:03 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.439843Z","level":"info","event":"25/12/31 03:02:03 INFO SparkContext: Submitted application: ListingsBookingsJoin","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.469463Z","level":"info","event":"25/12/31 03:02:03 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.469736Z","level":"info","event":"25/12/31 03:02:03 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.470088Z","level":"info","event":"25/12/31 03:02:03 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.470426Z","level":"info","event":"25/12/31 03:02:03 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.470862Z","level":"info","event":"25/12/31 03:02:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.624739Z","level":"info","event":"25/12/31 03:02:03 INFO Utils: Successfully started service 'sparkDriver' on port 44789.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.640722Z","level":"info","event":"25/12/31 03:02:03 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.647413Z","level":"info","event":"25/12/31 03:02:03 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.656954Z","level":"info","event":"25/12/31 03:02:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.657299Z","level":"info","event":"25/12/31 03:02:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.659451Z","level":"info","event":"25/12/31 03:02:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.673771Z","level":"info","event":"25/12/31 03:02:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bf32f198-8644-4e9f-9936-c534d067a96b","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.687300Z","level":"info","event":"25/12/31 03:02:03 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.762723Z","level":"info","event":"25/12/31 03:02:03 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.813219Z","level":"info","event":"25/12/31 03:02:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.848456Z","level":"info","event":"25/12/31 03:02:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.849700Z","level":"info","event":"25/12/31 03:02:03 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.850109Z","level":"info","event":"25/12/31 03:02:03 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.857152Z","level":"info","event":"25/12/31 03:02:03 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.857327Z","level":"info","event":"25/12/31 03:02:03 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.857520Z","level":"info","event":"25/12/31 03:02:03 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.857593Z","level":"info","event":"25/12/31 03:02:03 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.857821Z","level":"info","event":"25/12/31 03:02:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.928640Z","level":"info","event":"25/12/31 03:02:03 INFO Executor: Starting executor ID driver on host 3a15b831ef09","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.928974Z","level":"info","event":"25/12/31 03:02:03 INFO Executor: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.929191Z","level":"info","event":"25/12/31 03:02:03 INFO Executor: Java version 17.0.17+10-Debian-1deb12u1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.932785Z","level":"info","event":"25/12/31 03:02:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.933139Z","level":"info","event":"25/12/31 03:02:03 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2821e907 for default.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.944250Z","level":"info","event":"25/12/31 03:02:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37777.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.948038Z","level":"info","event":"25/12/31 03:02:03 INFO NettyBlockTransferService: Server created on 3a15b831ef09:37777","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.949136Z","level":"info","event":"25/12/31 03:02:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.958175Z","level":"info","event":"25/12/31 03:02:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3a15b831ef09, 37777, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.962342Z","level":"info","event":"25/12/31 03:02:03 INFO BlockManagerMasterEndpoint: Registering block manager 3a15b831ef09:37777 with 434.4 MiB RAM, BlockManagerId(driver, 3a15b831ef09, 37777, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.964016Z","level":"info","event":"25/12/31 03:02:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3a15b831ef09, 37777, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:03.964639Z","level":"info","event":"25/12/31 03:02:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3a15b831ef09, 37777, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:05.355985Z","level":"info","event":"25/12/31 03:02:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:05.360204Z","level":"info","event":"25/12/31 03:02:05 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:05.844123Z","level":"info","event":"25/12/31 03:02:05 INFO InMemoryFileIndex: It took 51 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:05.887099Z","level":"info","event":"25/12/31 03:02:05 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:05.905649Z","level":"info","event":"25/12/31 03:02:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 215.6 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:05.939322Z","level":"info","event":"25/12/31 03:02:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.9 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:05.943393Z","level":"info","event":"25/12/31 03:02:05 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.035267Z","level":"info","event":"25/12/31 03:02:06 INFO FileInputFormat: Total input files to process : 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.052687Z","level":"info","event":"25/12/31 03:02:06 INFO FileInputFormat: Total input files to process : 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.094642Z","level":"info","event":"25/12/31 03:02:06 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.103211Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.103530Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.103964Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.105272Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.107205Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Missing parents found for ResultStage 0: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.108301Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.120407Z","level":"info","event":"25/12/31 03:02:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.7 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.122246Z","level":"info","event":"25/12/31 03:02:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.123118Z","level":"info","event":"25/12/31 03:02:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.134331Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.136495Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.155354Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 9772 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.163848Z","level":"info","event":"25/12/31 03:02:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.206607Z","level":"info","event":"25/12/31 03:02:06 INFO BinaryFileRDD: Task (TID 0) input split: Paths:/opt/airflow/tmp/airbnb_data/listings.csv.gz:0+9744878","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.283174Z","level":"info","event":"25/12/31 03:02:06 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.317483Z","level":"info","event":"25/12/31 03:02:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2702 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.322670Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 177 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.323803Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.326399Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 210 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.328159Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.328418Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSchedulerImpl: Canceling stage 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.329223Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.330354Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 235.592337 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.353050Z","level":"info","event":"25/12/31 03:02:06 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.353627Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.353703Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.354112Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.354180Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.354623Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Missing parents found for ResultStage 1: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.354881Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.356802Z","level":"info","event":"25/12/31 03:02:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.9 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.358350Z","level":"info","event":"25/12/31 03:02:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.359423Z","level":"info","event":"25/12/31 03:02:06 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.360640Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.360939Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.361869Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 9772 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.362347Z","level":"info","event":"25/12/31 03:02:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.371764Z","level":"info","event":"25/12/31 03:02:06 INFO BinaryFileRDD: Task (TID 1) input split: Paths:/opt/airflow/tmp/airbnb_data/listings.csv.gz:0+9744878","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.404171Z","level":"info","event":"25/12/31 03:02:06 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.990515Z","level":"info","event":"25/12/31 03:02:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2084 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.994082Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 632 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.994222Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.995054Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 639 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.995489Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.995750Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSchedulerImpl: Canceling stage 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.995851Z","level":"info","event":"25/12/31 03:02:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:06.996866Z","level":"info","event":"25/12/31 03:02:06 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 643.758714 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:07.210525Z","level":"info","event":"25/12/31 03:02:07 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:07.281704Z","level":"info","event":"25/12/31 03:02:07 INFO InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:07.656582Z","level":"info","event":"25/12/31 03:02:07 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:07.658305Z","level":"info","event":"25/12/31 03:02:07 INFO FileSourceStrategy: Post-Scan Filters: Set((length(trim(value#79, None)) > 0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:07.946839Z","level":"info","event":"25/12/31 03:02:07 INFO CodeGenerator: Code generated in 145.649825 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:07.951980Z","level":"info","event":"25/12/31 03:02:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 214.5 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:07.959529Z","level":"info","event":"25/12/31 03:02:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:07.961026Z","level":"info","event":"25/12/31 03:02:07 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:07.971459Z","level":"info","event":"25/12/31 03:02:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4195863 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.001586Z","level":"info","event":"25/12/31 03:02:08 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.002469Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.002776Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.002877Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.003014Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.003498Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Missing parents found for ResultStage 2: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.003822Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.016920Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.3 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.018484Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.019655Z","level":"info","event":"25/12/31 03:02:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.020949Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.021110Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.025123Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.025941Z","level":"info","event":"25/12/31 03:02:08 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.059529Z","level":"info","event":"25/12/31 03:02:08 INFO CodeGenerator: Code generated in 12.058605 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.065917Z","level":"info","event":"25/12/31 03:02:08 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2025-12-31_0302/bookings.csv, range: 0-1559, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.080947Z","level":"info","event":"25/12/31 03:02:08 INFO CodeGenerator: Code generated in 9.621261 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.177366Z","level":"info","event":"25/12/31 03:02:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1727 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.179562Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 156 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.179696Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.180260Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 176 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.180880Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.180992Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Canceling stage 2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.181358Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.181643Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 179.897345 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.202118Z","level":"info","event":"25/12/31 03:02:08 INFO CodeGenerator: Code generated in 9.957761 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.233574Z","level":"info","event":"25/12/31 03:02:08 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.233757Z","level":"info","event":"25/12/31 03:02:08 INFO FileSourceStrategy: Post-Scan Filters: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.241908Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.5 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.248530Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.250479Z","level":"info","event":"25/12/31 03:02:08 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.251699Z","level":"info","event":"25/12/31 03:02:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4195863 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.278518Z","level":"info","event":"25/12/31 03:02:08 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.279990Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.280158Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.280230Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.280290Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.281173Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Missing parents found for ResultStage 3: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.281506Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.292016Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 22.9 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.293302Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.294695Z","level":"info","event":"25/12/31 03:02:08 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.296218Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.296749Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.298643Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.299510Z","level":"info","event":"25/12/31 03:02:08 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.323428Z","level":"info","event":"25/12/31 03:02:08 INFO CodeGenerator: Code generated in 7.045209 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.325514Z","level":"info","event":"25/12/31 03:02:08 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2025-12-31_0302/bookings.csv, range: 0-1559, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.410796Z","level":"info","event":"25/12/31 03:02:08 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1927 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.412803Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 115 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.412920Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Removed TaskSet 3.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.413448Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 132 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.413561Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.413598Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Canceling stage 3","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.413731Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.413964Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 135.455653 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.417126Z","level":"info","event":"Listings schema:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420289Z","level":"info","event":"root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420432Z","level":"info","event":"|-- id: long (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420501Z","level":"info","event":"|-- listing_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420552Z","level":"info","event":"|-- scrape_id: long (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420591Z","level":"info","event":"|-- last_scraped: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420626Z","level":"info","event":"|-- source: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420661Z","level":"info","event":"|-- name: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420696Z","level":"info","event":"|-- description: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420731Z","level":"info","event":"|-- neighborhood_overview: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420768Z","level":"info","event":"|-- picture_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420809Z","level":"info","event":"|-- host_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420848Z","level":"info","event":"|-- host_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420884Z","level":"info","event":"|-- host_name: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420918Z","level":"info","event":"|-- host_since: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420952Z","level":"info","event":"|-- host_location: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.420985Z","level":"info","event":"|-- host_about: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421019Z","level":"info","event":"|-- host_response_time: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421085Z","level":"info","event":"|-- host_response_rate: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421127Z","level":"info","event":"|-- host_acceptance_rate: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421164Z","level":"info","event":"|-- host_is_superhost: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421199Z","level":"info","event":"|-- host_thumbnail_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421234Z","level":"info","event":"|-- host_picture_url: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421268Z","level":"info","event":"|-- host_neighbourhood: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421302Z","level":"info","event":"|-- host_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421337Z","level":"info","event":"|-- host_total_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421371Z","level":"info","event":"|-- host_verifications: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421405Z","level":"info","event":"|-- host_has_profile_pic: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421440Z","level":"info","event":"|-- host_identity_verified: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421475Z","level":"info","event":"|-- neighbourhood: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421509Z","level":"info","event":"|-- neighbourhood_cleansed: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421543Z","level":"info","event":"|-- neighbourhood_group_cleansed: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421577Z","level":"info","event":"|-- latitude: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421612Z","level":"info","event":"|-- longitude: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421646Z","level":"info","event":"|-- property_type: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421679Z","level":"info","event":"|-- room_type: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421715Z","level":"info","event":"|-- accommodates: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421748Z","level":"info","event":"|-- bathrooms: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421782Z","level":"info","event":"|-- bathrooms_text: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421817Z","level":"info","event":"|-- bedrooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421852Z","level":"info","event":"|-- beds: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421889Z","level":"info","event":"|-- amenities: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421927Z","level":"info","event":"|-- price: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.421998Z","level":"info","event":"|-- minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422060Z","level":"info","event":"|-- maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422163Z","level":"info","event":"|-- minimum_minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422189Z","level":"info","event":"|-- maximum_minimum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422206Z","level":"info","event":"|-- minimum_maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422220Z","level":"info","event":"|-- maximum_maximum_nights: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422234Z","level":"info","event":"|-- minimum_nights_avg_ntm: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422247Z","level":"info","event":"|-- maximum_nights_avg_ntm: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422259Z","level":"info","event":"|-- calendar_updated: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422273Z","level":"info","event":"|-- has_availability: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422286Z","level":"info","event":"|-- availability_30: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422299Z","level":"info","event":"|-- availability_60: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422312Z","level":"info","event":"|-- availability_90: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422325Z","level":"info","event":"|-- availability_365: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422337Z","level":"info","event":"|-- calendar_last_scraped: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422356Z","level":"info","event":"|-- number_of_reviews: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422369Z","level":"info","event":"|-- number_of_reviews_ltm: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422382Z","level":"info","event":"|-- number_of_reviews_l30d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422396Z","level":"info","event":"|-- availability_eoy: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422409Z","level":"info","event":"|-- number_of_reviews_ly: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422422Z","level":"info","event":"|-- estimated_occupancy_l365d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422436Z","level":"info","event":"|-- estimated_revenue_l365d: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422449Z","level":"info","event":"|-- first_review: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422462Z","level":"info","event":"|-- last_review: date (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422475Z","level":"info","event":"|-- review_scores_rating: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422488Z","level":"info","event":"|-- review_scores_accuracy: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422500Z","level":"info","event":"|-- review_scores_cleanliness: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422513Z","level":"info","event":"|-- review_scores_checkin: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422526Z","level":"info","event":"|-- review_scores_communication: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422539Z","level":"info","event":"|-- review_scores_location: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422552Z","level":"info","event":"|-- review_scores_value: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422565Z","level":"info","event":"|-- license: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422578Z","level":"info","event":"|-- instant_bookable: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422591Z","level":"info","event":"|-- calculated_host_listings_count: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422605Z","level":"info","event":"|-- calculated_host_listings_count_entire_homes: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422619Z","level":"info","event":"|-- calculated_host_listings_count_private_rooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422632Z","level":"info","event":"|-- calculated_host_listings_count_shared_rooms: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422645Z","level":"info","event":"|-- reviews_per_month: double (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422658Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422673Z","level":"info","event":"Bookings schema:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422695Z","level":"info","event":"root","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422709Z","level":"info","event":"|-- booking_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422722Z","level":"info","event":"|-- listing_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422735Z","level":"info","event":"|-- user_id: integer (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422748Z","level":"info","event":"|-- booking_time: timestamp (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422760Z","level":"info","event":"|-- status: string (nullable = true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.422773Z","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.637569Z","level":"info","event":"25/12/31 03:02:08 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.637717Z","level":"info","event":"25/12/31 03:02:08 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(id#0L))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.639156Z","level":"info","event":"25/12/31 03:02:08 INFO FileSourceStrategy: Pushed Filters: IsNotNull(listing_id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.639295Z","level":"info","event":"25/12/31 03:02:08 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(listing_id#97))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.735864Z","level":"info","event":"25/12/31 03:02:08 INFO CodeGenerator: Code generated in 7.933727 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.738828Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 214.4 KiB, free 433.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.745176Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 38.7 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.746678Z","level":"info","event":"25/12/31 03:02:08 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.750643Z","level":"info","event":"25/12/31 03:02:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4195863 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.761949Z","level":"info","event":"25/12/31 03:02:08 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.763057Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.763167Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.763239Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.763473Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.763744Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Missing parents found for ResultStage 4: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.763909Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.766078Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.4 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.767644Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 433.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.768999Z","level":"info","event":"25/12/31 03:02:08 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.769891Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.770083Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.771605Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10260 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.772139Z","level":"info","event":"25/12/31 03:02:08 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.793111Z","level":"info","event":"25/12/31 03:02:08 INFO CodeGenerator: Code generated in 14.234109 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.794088Z","level":"info","event":"25/12/31 03:02:08 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/data/bookings/2025-12-31_0302/bookings.csv, range: 0-1559, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.807239Z","level":"info","event":"25/12/31 03:02:08 INFO CodeGenerator: Code generated in 11.340133 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.830272Z","level":"info","event":"25/12/31 03:02:08 INFO CodeGenerator: Code generated in 4.932967 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.907853Z","level":"info","event":"25/12/31 03:02:08 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2064 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.910207Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 138 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.910441Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Removed TaskSet 4.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.910889Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 146 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.911178Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.911242Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Canceling stage 4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.911267Z","level":"info","event":"25/12/31 03:02:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.911458Z","level":"info","event":"25/12/31 03:02:08 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 149.419741 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.929215Z","level":"info","event":"25/12/31 03:02:08 INFO CodeGenerator: Code generated in 8.612301 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.931899Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 1024.5 KiB, free 433.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.936017Z","level":"info","event":"25/12/31 03:02:08 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 737.0 B, free 433.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.938421Z","level":"info","event":"25/12/31 03:02:08 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.950681Z","level":"info","event":"25/12/31 03:02:08 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:08.950964Z","level":"info","event":"25/12/31 03:02:08 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(id#0L))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.076754Z","level":"info","event":"25/12/31 03:02:09 INFO CodeGenerator: Code generated in 45.420066 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.079494Z","level":"info","event":"25/12/31 03:02:09 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 215.1 KiB, free 432.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.085064Z","level":"info","event":"25/12/31 03:02:09 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 432.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.086619Z","level":"info","event":"25/12/31 03:02:09 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.088383Z","level":"info","event":"25/12/31 03:02:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13939182 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.130176Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: Registering RDD 21 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.134167Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: Got map stage job 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.134518Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: Final stage: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.135428Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.136430Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.137269Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: Missing parents found for ShuffleMapStage 5: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.137496Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.147566Z","level":"info","event":"25/12/31 03:02:09 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 55.7 KiB, free 432.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.149132Z","level":"info","event":"25/12/31 03:02:09 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 432.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.150608Z","level":"info","event":"25/12/31 03:02:09 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.152040Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.152153Z","level":"info","event":"25/12/31 03:02:09 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.153668Z","level":"info","event":"25/12/31 03:02:09 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (3a15b831ef09,executor driver, partition 0, PROCESS_LOCAL, 10234 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.154281Z","level":"info","event":"25/12/31 03:02:09 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.205198Z","level":"info","event":"25/12/31 03:02:09 INFO CodeGenerator: Code generated in 38.660474 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.232733Z","level":"info","event":"25/12/31 03:02:09 INFO CodeGenerator: Code generated in 17.977181 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.241724Z","level":"info","event":"25/12/31 03:02:09 INFO SecurityManager: Changing view acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.242009Z","level":"info","event":"25/12/31 03:02:09 INFO SecurityManager: Changing modify acls to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.242112Z","level":"info","event":"25/12/31 03:02:09 INFO SecurityManager: Changing view acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.242178Z","level":"info","event":"25/12/31 03:02:09 INFO SecurityManager: Changing modify acls groups to: default","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.242228Z","level":"info","event":"25/12/31 03:02:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY; RPC SSL disabled","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.262033Z","level":"info","event":"25/12/31 03:02:09 INFO CodeGenerator: Code generated in 4.605876 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.278251Z","level":"info","event":"25/12/31 03:02:09 INFO CodeGenerator: Code generated in 4.972859 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.287531Z","level":"info","event":"25/12/31 03:02:09 INFO CodeGenerator: Code generated in 4.643813 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.292385Z","level":"info","event":"25/12/31 03:02:09 INFO FileScanRDD: Reading File path: file:///opt/airflow/tmp/airbnb_data/listings.csv.gz, range: 0-9744878, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.300023Z","level":"info","event":"25/12/31 03:02:09 INFO CodeGenerator: Code generated in 5.61844 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.307644Z","level":"info","event":"25/12/31 03:02:09 INFO CodeGenerator: Code generated in 3.147546 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.351705Z","level":"info","event":"25/12/31 03:02:09 INFO CodecPool: Got brand-new decompressor [.gz]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.882754Z","level":"info","event":"25/12/31 03:02:09 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3512 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.884506Z","level":"info","event":"25/12/31 03:02:09 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 732 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.884711Z","level":"info","event":"25/12/31 03:02:09 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.886175Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 746 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.886504Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.886648Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: running: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.886962Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: waiting: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.887250Z","level":"info","event":"25/12/31 03:02:09 INFO DAGScheduler: failed: HashSet()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.894444Z","level":"info","event":"25/12/31 03:02:09 INFO ShufflePartitionsUtil: For shuffle(0, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.936918Z","level":"info","event":"25/12/31 03:02:09 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.937599Z","level":"info","event":"25/12/31 03:02:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.937667Z","level":"info","event":"25/12/31 03:02:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:09.938707Z","level":"info","event":"25/12/31 03:02:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.134523Z","level":"info","event":"25/12/31 03:02:10 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.152104Z","level":"info","event":"25/12/31 03:02:10 INFO CodeGenerator: Code generated in 10.64148 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.188174Z","level":"info","event":"25/12/31 03:02:10 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.189858Z","level":"info","event":"25/12/31 03:02:10 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.190059Z","level":"info","event":"25/12/31 03:02:10 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.190143Z","level":"info","event":"25/12/31 03:02:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.190201Z","level":"info","event":"25/12/31 03:02:10 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.190464Z","level":"info","event":"25/12/31 03:02:10 INFO DAGScheduler: Missing parents found for ResultStage 7: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.190606Z","level":"info","event":"25/12/31 03:02:10 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.208783Z","level":"info","event":"25/12/31 03:02:10 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 281.1 KiB, free 432.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.210387Z","level":"info","event":"25/12/31 03:02:10 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 104.1 KiB, free 432.5 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.211222Z","level":"info","event":"25/12/31 03:02:10 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1701","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.211874Z","level":"info","event":"25/12/31 03:02:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.211959Z","level":"info","event":"25/12/31 03:02:10 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.216517Z","level":"info","event":"25/12/31 03:02:10 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (3a15b831ef09,executor driver, partition 0, NODE_LOCAL, 9978 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.217082Z","level":"info","event":"25/12/31 03:02:10 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.239267Z","level":"info","event":"25/12/31 03:02:10 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.239512Z","level":"info","event":"25/12/31 03:02:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.239622Z","level":"info","event":"25/12/31 03:02:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.239874Z","level":"info","event":"25/12/31 03:02:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.523484Z","level":"info","event":"25/12/31 03:02:10 INFO ShuffleBlockFetcherIterator: Getting 1 (1032.0 B) non-empty blocks including 1 (1032.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.526656Z","level":"info","event":"25/12/31 03:02:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.542560Z","level":"info","event":"25/12/31 03:02:10 INFO CodeGenerator: Code generated in 12.830899 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.718998Z","level":"info","event":"25/12/31 03:02:10 INFO FileOutputCommitter: Saved output of task 'attempt_202512310302108513675514251253059_0007_m_000000_6' to file:/opt/airflow/tmp/data/bookings_per_listing/2025-12-31_0302/_temporary/0/task_202512310302108513675514251253059_0007_m_000000","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.720257Z","level":"info","event":"25/12/31 03:02:10 INFO SparkHadoopMapRedUtil: attempt_202512310302108513675514251253059_0007_m_000000_6: Committed. Elapsed time: 53 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.726341Z","level":"info","event":"25/12/31 03:02:10 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 6794 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.727633Z","level":"info","event":"25/12/31 03:02:10 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 513 ms on 3a15b831ef09 (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.727756Z","level":"info","event":"25/12/31 03:02:10 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.728616Z","level":"info","event":"25/12/31 03:02:10 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 538 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.728853Z","level":"info","event":"25/12/31 03:02:10 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.729120Z","level":"info","event":"25/12/31 03:02:10 INFO TaskSchedulerImpl: Canceling stage 7","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.729191Z","level":"info","event":"25/12/31 03:02:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.729697Z","level":"info","event":"25/12/31 03:02:10 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 541.502351 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:10.730873Z","level":"info","event":"25/12/31 03:02:10 INFO FileFormatWriter: Start to commit write Job 5b654db4-95b9-4422-9226-fc21ea063e5d.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:11.100555Z","level":"info","event":"25/12/31 03:02:11 INFO FileFormatWriter: Write Job 5b654db4-95b9-4422-9226-fc21ea063e5d committed. Elapsed time: 368 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:11.102793Z","level":"info","event":"25/12/31 03:02:11 INFO FileFormatWriter: Finished processing stats for write job 5b654db4-95b9-4422-9226-fc21ea063e5d.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:11.110327Z","level":"info","event":"Aggregated results written to /opt/airflow/tmp/data/bookings_per_listing/2025-12-31_0302","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:11.111071Z","level":"info","event":"25/12/31 03:02:11 INFO SparkContext: SparkContext is stopping with exitCode 0 from stop at NativeMethodAccessorImpl.java:0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:11.133075Z","level":"info","event":"25/12/31 03:02:11 INFO SparkUI: Stopped Spark web UI at http://3a15b831ef09:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:11.140683Z","level":"info","event":"25/12/31 03:02:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:11.150246Z","level":"info","event":"25/12/31 03:02:11 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:11.150423Z","level":"info","event":"25/12/31 03:02:11 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:11.152512Z","level":"info","event":"25/12/31 03:02:11 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:11.154464Z","level":"info","event":"25/12/31 03:02:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
{"timestamp":"2025-12-31T03:02:11.161060Z","level":"info","event":"25/12/31 03:02:11 INFO SparkContext: Successfully stopped SparkContext (Uptime: 7830 ms)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":643}
